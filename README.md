# ğŸ§  DocScribe â€” Voice to Structured Clinical Notes  
**Gator Hack IV | AI in Healthcare**

> â€œWhen clinicians speak, the record writes itself.â€  
> *DocScribe* transforms clinician voice input into structured medical documentation â€” generating clean, explainable, and EMR-ready notes in seconds.

---

## ğŸŒŸ Overview
Healthcare professionals spend up to **40â€“50%** of their workday documenting care.  
**DocScribe** reduces this burden by using AI to automatically:
- ğŸ™ï¸ Transcribe spoken diagnostic reasoning (via Whisper)  
- ğŸ§© Extract structured fields like diagnosis, orders, and plan (via Flan-T5)  
- ğŸ“‹ Generate standardized SOAP notes and patient summaries  
- ğŸ” Highlight transcript phrases that support each section  
- ğŸ“¤ Export ready-to-review notes in JSON, Markdown, or PDF  


---

## ğŸ§± Architecture

```text
ğŸ¤ Voice Input (Clinician Dictation)
     â†“ Whisper (ASR)
ğŸ“ Transcript (editable)
     â†“ Flan-T5 Extraction (Few-shot prompt)
{ chief_complaint, assessment, diagnosis[], orders[], plan[], follow_up }
     â†“
ğŸ“‹ Note Composer (SOAP + Patient Summary)
     â†“
ğŸ” Span Highlighter â†’ Traceable Output
     â†“
â¬‡ Export (JSON / PDF / Markdown)


## ğŸ§  Core Features

| Feature | Description |
|----------|--------------|
| ğŸ™ï¸ **Speech-to-Text** | Whisper converts spoken dictation into text |
| ğŸ§© **Structured Extraction** | LLM extracts JSON with clear clinical fields |
| ğŸ“‹ **SOAP Note Generator** | Automatically formats clinician notes (S/O/A/P sections) |
| ğŸ” **Explainable Output** | Highlights transcript phrases used in each note section |
| ğŸ“¤ **Exports** | Generate JSON, Markdown, and PDF (with safety disclaimer) |
| âš–ï¸ **Responsible AI** | Guardrails prevent hallucinated medications or diagnoses |

## ğŸ“š Datasets

| Role | Dataset | Description | Source |
|------|----------|--------------|---------|
| **Primary** | [`abisee/medical_dialogue`](https://huggingface.co/datasets/abisee/medical_dialogue) | Doctorâ€“patient dictations with assessments & plans | Hugging Face |
| **Formatting** | [`medical_meadow/clinical_notes_synth`](https://huggingface.co/datasets/medical_meadow/clinical_notes_synth) | Synthetic SOAP/EMR-style notes | Hugging Face |
| **Optional** | [`openlifescienceai/medmcqa`](https://huggingface.co/datasets/openlifescienceai/medmcqa) | Clinical reasoning Q&A for decision support | Hugging Face |

## âš™ï¸ Installation

Follow the steps below to set up **DocScribe** on your system.  
Supports both **macOS** ğŸ–¥ï¸ and **Windows** ğŸ’» environments.

---

### ğŸ§© Step 1 â€” Clone the Repository
```bash
git clone https://github.com/<your-username>/docscribe.git
cd docscribe
```
### ğŸ macOS Setup

1. Create a Virtual Environment
```bash
python3 -m venv .venv
source .venv/bin/activate
```
2. Install Dependencies
```bash
pip install --upgrade pip
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
pip install transformers datasets accelerate pydantic streamlit openai-whisper reportlab sounddevice numpy
```

3. Install FFmpeg (required for Whisper)
```bash
brew install ffmpeg
```
ğŸ’¡ If you donâ€™t have Homebrew installed:
```bash
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
```
ğŸªŸ Windows Setup
1. Create a Virtual Environment
```bash
python -m venv .venv
.venv\Scripts\activate
```

2. Install Dependencies
```bash
pip install --upgrade pip
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
pip install transformers datasets accelerate pydantic streamlit openai-whisper reportlab sounddevice numpy
```

3. Install FFmpeg (required for Whisper)
	1.	Download FFmpeg from the official website: https://ffmpeg.org/download.html
	2.	Extract it and copy the path to the bin folder.
	3.	Add that path to your System Environment Variables â†’ PATH.
	4.	Restart your terminal or VS Code before continuing.
	
### Step 4 â€” Verify Installation	
```bash
python -c "import torch, whisper; print('PyTorch:', torch.__version__); print('Whisper OK âœ…')"
```
Expected Output:
```bash
PyTorch: <version>
Whisper OK âœ…
```
If you see this message, your environment is ready!

### Step 5 â€” Run DocScribe

Test the clinical extractor and composer:
```bash
python src/extract_clinical.py
python src/compose_note.py
```
Launch the Streamlit App (after UI integration):
Thatâ€™s it! ğŸ‰
You now have a fully working environment for DocScribe

ğŸ§© Quick Start (Command Line Demo)

Run the clinical extractor + SOAP composer from your terminal:
```bash
python -c "from src.extract_clinical import extract_info; \
from src.compose_note import compose_note; \
note = extract_info('Fever and cough for 3 days. Suspect pneumonia. Order chest X-ray and start azithromycin.'); \
print(compose_note(note)[0])"
```

Expected Output:
```bash
S: Fever and cough for 3 days.
O: Chest X-ray.
A: Suspected pneumonia (Pneumonia).
P: Start azithromycin 500 mg daily.
Follow-up: Re-evaluate in 2 days.
```

### ğŸ’» Run the Streamlit App
```bash
streamlit run app.py
```
Youâ€™ll see:
	â€¢	ğŸ™ï¸ Audio upload or record
	â€¢	ğŸ§© Structured JSON
	â€¢	ğŸ“‹ SOAP Note + Patient Summary
	â€¢	ğŸ” Highlighted transcript phrases
	â€¢	ğŸ“¤ Export buttons
	
## ğŸ§ª Evaluation (Example)

| Metric | Result | Notes |
|---------|--------|-------|
| **Diagnosis F1** | 0.88 | Evaluated on 15 test cases |
| **Orders F1** | 0.85 | From synthetic transcripts |
| **Latency** | 4.3 s | End-to-end: Audio â†’ JSON â†’ SOAP |
| **Hallucination Rate** | 0 % | Guardrails successfully applied |

## ğŸ“ Project Structure
```
docscribe/
â”œâ”€ app.py                      # Streamlit UI 
â”œâ”€ prompts/
â”‚  â”œâ”€ extractor_fewshot.md     # Few-shot examples for extraction
â”‚  â””â”€ soap_fewshot.md          # SOAP layout exemplars
â”œâ”€ src/
â”‚  â”œâ”€ extract_clinical.py      # LLM extractor
â”‚  â”œâ”€ compose_note.py          # SOAP & summary composer
â”‚  â”œâ”€ schema.py                # Pydantic schema
â”‚  â”œâ”€ asr_whisper.py           # Audio transcription 
â”‚  â””â”€ highlight_spans.py       # Keyword highlighter 
â”œâ”€ data/
â”‚  â””â”€ samples_audio/           # Demo recordings (.wav)
â”œâ”€ eval/
â”‚  â”œâ”€ eval_transcripts.jsonl   # Evaluation set
â”‚  â””â”€ run_eval.py              # F1 scoring script
â””â”€ README.md                   # This file
```

ğŸ§© Example Audio Scripts

1ï¸âƒ£ Pneumonia

â€œFever and cough for three days with mild shortness of breath. I suspect community-acquired pneumonia. Order chest X-ray and start azithromycin five hundred milligrams daily. Follow-up in two days.â€

2ï¸âƒ£ Ankle Sprain

â€œLeft ankle pain after inversion injury yesterday. Likely lateral ankle sprain. X-ray ankle to rule out fracture. RICE and ibuprofen four hundred milligrams as needed.â€

3ï¸âƒ£ UTI

â€œDysuria and urinary frequency for two days. No fever or flank pain. Likely uncomplicated UTI. Urinalysis and nitrofurantoin one hundred milligrams twice daily for five days.â€

ğŸ¯ Roadmap
	â€¢	Audio â†’ Transcript pipeline
	â€¢	JSON extraction with Flan-T5
	â€¢	SOAP note composer
	â€¢	Streamlit UI (in progress)
	â€¢	Span-level highlighting
	â€¢	ICD-10 auto-coding (top 50)
	â€¢	Bilingual mode (EN â†” ES)


âš–ï¸ Ethics & Limitations
	â€¢	No real patient data used.
	â€¢	Outputs are drafts, not clinical decisions.
	â€¢	Always review and verify before use in practice.
	â€¢	For research and hackathon demonstration only.

ğŸ‘¥ Team Roles
| Member | Role | Focus |
|---------|------|--------|
| **[Deepika Sarala Pratapa]** | AI & Clinical Intelligence Lead | Prompt design, LLM extraction, evaluation |
| **[Rohit Bogulla]** | Full-Stack & UI/UX Lead | Whisper integration, Streamlit design, highlighting, exports |

ğŸ“œ License

MIT License Â© 2025 THEDIVERGENTS
For academic and research use only.

â¸»

ğŸ©º Acknowledgments
	â€¢	Hugging Face Datasets: abisee/medical_dialogue, medical_meadow/clinical_notes_synth
	â€¢	Models: google/flan-t5-base, openai/whisper-tiny
	â€¢	Hackathon: Gator Hack IV â€“ AI Days, University of Florida













