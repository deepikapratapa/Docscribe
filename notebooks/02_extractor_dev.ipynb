{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9aec9d5-23e8-40e9-ba26-c39f14c52d47",
   "metadata": {},
   "source": [
    "# ðŸ§  DocScribe â€” 02 Â· Extractor Development (Public, No Hardcoding)\n",
    "\n",
    "- Model: `google/flan-t5-large` (public)\n",
    "- Prompt: schema-only (no clinical examples)\n",
    "- Parsing: per-key boundary parsing + grounding (verbatim / relaxed match)\n",
    "- Backoff: per-field micro-prompts (extractive) + minimal generic regex salvage\n",
    "- Routing: imaging/labs â†’ Orders; dosed meds â†’ Plan (mirrored to Orders for demo)\n",
    "- Fixes: verb stripping for both targets, PRN not treated as follow-up, robust split with relaxed containment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4cfa035-e399-4f17-a3c0-a588f547daa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /Users/saturnine/DocScribe\n",
      "SRC : /Users/saturnine/DocScribe/src\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# If this notebook lives in <repo>/notebooks/, ROOT is the repo root.\n",
    "NB_DIR = Path.cwd()\n",
    "if NB_DIR.name.lower() != \"notebooks\":\n",
    "    # fallback: look upward for a \"notebooks\" folder\n",
    "    probe = NB_DIR\n",
    "    for _ in range(4):\n",
    "        if (probe / \"notebooks\").exists():\n",
    "            break\n",
    "        probe = probe.parent\n",
    "    ROOT = probe\n",
    "else:\n",
    "    ROOT = NB_DIR.parent\n",
    "\n",
    "SRC = ROOT / \"src\"\n",
    "SRC.mkdir(exist_ok=True)\n",
    "\n",
    "# Make 'src' importable in *this* kernel too (not required for 03, but handy)\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"SRC :\", SRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e95bf1-75b2-4333-a280-cbaf9f1b9fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Device: CPU\n",
      "ðŸ§© Model: google/flan-t5-large\n",
      "ðŸ”„ Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/docscribe/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model ready.\n"
     ]
    }
   ],
   "source": [
    "import os, re, json, time, torch\n",
    "from typing import Dict, Any, List, Tuple\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "torch.manual_seed(42)\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "DEVICE = 0 if torch.cuda.is_available() else -1\n",
    "MODEL_NAME = os.environ.get(\"DOCSCRIBE_MODEL\", \"google/flan-t5-large\")\n",
    "\n",
    "print(\"âœ… Device:\", \"GPU\" if DEVICE >= 0 else \"CPU\")\n",
    "print(\"ðŸ§© Model:\", MODEL_NAME)\n",
    "\n",
    "# Deterministic gen; for faster CPU demos set num_beams=1, max_new_tokens=320\n",
    "GEN_KW = dict(\n",
    "    do_sample=False,\n",
    "    num_beams=4,\n",
    "    temperature=0.0,\n",
    "    max_new_tokens=420,\n",
    "    early_stopping=True,\n",
    ")\n",
    "\n",
    "print(\"ðŸ”„ Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "t5 = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=DEVICE)\n",
    "print(\"âœ… Model ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da2f5ef2-4e3b-4053-ad62-ab5c5bdb36a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClinicalNote(BaseModel):\n",
    "    chief_complaint: str = Field(default=\"\")\n",
    "    assessment: str = Field(default=\"\")\n",
    "    diagnosis: List[str] = Field(default_factory=list)\n",
    "    orders: List[str] = Field(default_factory=list)\n",
    "    plan: List[str] = Field(default_factory=list)\n",
    "    follow_up: str = Field(default=\"\")\n",
    "\n",
    "    def pretty(self) -> str:\n",
    "        return self.json(indent=2, ensure_ascii=False, exclude_none=True)\n",
    "\n",
    "def compose_soap(note: ClinicalNote) -> str:\n",
    "    s = note.chief_complaint or \"â€”\"\n",
    "    o = \", \".join(note.orders) if note.orders else \"â€”\"\n",
    "    a = note.assessment or (\", \".join(note.diagnosis) if note.diagnosis else \"â€”\")\n",
    "    p = \"; \".join(note.plan) if note.plan else \"â€”\"\n",
    "    f = note.follow_up or \"â€”\"\n",
    "    return f\"S: {s}\\nO: {o}\\nA: {a}\\nP: {p}\\nFollow-up: {f}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "066e6875-2f24-4e0e-bd0f-0114f8fad2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Prompt ready.\n"
     ]
    }
   ],
   "source": [
    "FEWSHOT = \"\"\"You are a documentation assistant.\n",
    "\n",
    "Return ONE valid JSON object ONLY. Start with '{' and end with '}'.\n",
    "Use EXACTLY these keys and types:\n",
    "- \"chief_complaint\": string\n",
    "- \"assessment\": string\n",
    "- \"diagnosis\": array of strings\n",
    "- \"orders\": array of strings\n",
    "- \"plan\": array of strings\n",
    "- \"follow_up\": string\n",
    "\n",
    "STRICT RULES:\n",
    "- Derive content ONLY from the TRANSCRIPT text.\n",
    "- Every value MUST be a verbatim substring of the TRANSCRIPT (case-insensitive allowed).\n",
    "- If a value is not present, leave it \"\" (for strings) or [] (for arrays).\n",
    "- Do NOT add any text before or after the JSON.\n",
    "\n",
    "TRANSCRIPT:\n",
    "{transcript}\n",
    "\n",
    "JSON:\n",
    "\"\"\"\n",
    "print(\"ðŸ“‹ Prompt ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e349c6d-eeb0-4558-abed-68f8c2634e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYS_ORDER = [\"chief_complaint\",\"assessment\",\"diagnosis\",\"orders\",\"plan\",\"follow_up\"]\n",
    "KEYS_SET   = set(KEYS_ORDER)\n",
    "KEY_START_RE = re.compile(r'(?:\"?(chief_complaint|assessment|diagnosis|orders|plan|follow_up)\"?\\s*:)', re.I)\n",
    "\n",
    "def _find_key_spans(txt: str) -> Dict[str, slice]:\n",
    "    spans, positions = {}, []\n",
    "    for m in KEY_START_RE.finditer(txt):\n",
    "        k = m.group(1).lower()\n",
    "        positions.append((k, m.start(), m.end()))\n",
    "    for i, (k, s, e) in enumerate(positions):\n",
    "        nxt = positions[i+1][1] if i+1 < len(positions) else len(txt)\n",
    "        spans[k] = slice(e, nxt)\n",
    "    return spans\n",
    "\n",
    "def _grab_string_val(chunk: str) -> str:\n",
    "    m = re.search(r'\"\\s*([^\"]*?)\\s*\"', chunk)  # \"value\"\n",
    "    if m: return m.group(1).strip()\n",
    "    m = re.search(r':\\s*([^,\\]\\}]+)', chunk)    # : value\n",
    "    return m.group(1).strip() if m else \"\"\n",
    "\n",
    "def _grab_list_val(chunk: str) -> List[str]:\n",
    "    m = re.search(r'\\[\\s*([^\\]]*?)\\s*\\]', chunk)\n",
    "    inside = m.group(1) if m else chunk\n",
    "    items = re.findall(r'\"([^\"]+)\"', inside) or [x.strip() for x in re.split(r'[;,]', inside) if x.strip()]\n",
    "    cleaned, seen = [], set()\n",
    "    for it in items:\n",
    "        it = it.strip()\n",
    "        if not it or it.lower() in KEYS_SET or len(it) <= 1:\n",
    "            continue\n",
    "        low = it.lower()\n",
    "        if low not in seen:\n",
    "            seen.add(low); cleaned.append(it)\n",
    "    return cleaned\n",
    "\n",
    "def parse_fields_with_boundaries(raw_txt: str) -> Dict[str, Any]:\n",
    "    t = (raw_txt or \"\").replace(\"â€œ\", '\"').replace(\"â€\", '\"').replace(\"â€™\", \"'\")\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "    # Try JSON first\n",
    "    mjson = re.search(r\"\\{[\\s\\S]*\\}\", t)\n",
    "    if mjson:\n",
    "        block = mjson.group(0)\n",
    "        try:\n",
    "            data = json.loads(block)\n",
    "            data = {k: v for k, v in data.items() if k in KEYS_SET}\n",
    "            for k in KEYS_ORDER:\n",
    "                data.setdefault(k, [] if k in (\"diagnosis\",\"orders\",\"plan\") else \"\")\n",
    "            return data\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Boundary parse\n",
    "    spans = _find_key_spans(t)\n",
    "    data: Dict[str, Any] = {k: ([] if k in (\"diagnosis\",\"orders\",\"plan\") else \"\") for k in KEYS_ORDER}\n",
    "    for k in KEYS_ORDER:\n",
    "        if k not in spans:\n",
    "            continue\n",
    "        chunk = t[spans[k]]\n",
    "        data[k] = _grab_list_val(chunk) if k in (\"diagnosis\",\"orders\",\"plan\") else _grab_string_val(chunk)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae266536-b96d-499e-8de5-e004efb9c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_LEAD_VERBS = re.compile(r\"^\\s*(?:start|begin|initiate|recommend|advise|continue|order|obtain|get|perform|schedule)\\s+\", re.IGNORECASE)\n",
    "_DETERMINERS = re.compile(r\"^\\s*(?:to|the|a|an)\\s+\", re.IGNORECASE)\n",
    "\n",
    "def _canonical(s: str) -> str:\n",
    "    if not s: \n",
    "        return \"\"\n",
    "    x = s.strip().rstrip(\".\")\n",
    "    x = _LEAD_VERBS.sub(\"\", x)\n",
    "    x = _DETERMINERS.sub(\"\", x)\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "    x = re.sub(r\"\\bx(\\d+)\\s*day\\b\", r\"x\\1 days\", x, flags=re.IGNORECASE)  # dayâ†’days\n",
    "    return x.lower()\n",
    "\n",
    "# -------- Diagnosis de-hedging --------\n",
    "_HEDGES = re.compile(r\"\\b(likely|suspected|suspect|possible|probable|prob|consistent with|r/o|rule out)\\b[:\\s,-]*\", re.IGNORECASE)\n",
    "def _dehedge(s: str) -> str:\n",
    "    return _HEDGES.sub(\"\", s or \"\").strip(\" .,:;\")\n",
    "\n",
    "# -------- Imaging/lab modality normalization --------\n",
    "_MODALITY_CANON = {\n",
    "    r\"(?:x[\\-\\s]?ray|xr|xray)\": \"X-ray\",\n",
    "    r\"(?:ct\\s*scan|ct)\": \"CT\",\n",
    "    r\"(?:mri)\": \"MRI\",\n",
    "    r\"(?:ultra\\s*sound|us)\": \"Ultrasound\",\n",
    "    r\"(?:ekg|ecg)\": \"ECG\",\n",
    "    r\"(?:echo|echocardiogram)\": \"Echo\",\n",
    "}\n",
    "def _normalize_order_phrase(s: str) -> str:\n",
    "    \"\"\"Flip '<modality> <target>' â†’ '<target> <Modality>' and tidy spacing/case.\"\"\"\n",
    "    txt = re.sub(r\"\\s+\", \" \", (s or \"\")).strip().rstrip(\".\")\n",
    "    for mod_re, canon in _MODALITY_CANON.items():\n",
    "        m = re.match(rf\"(?i)^{mod_re}\\s+(.+)$\", txt)\n",
    "        if m:\n",
    "            target = m.group(1).strip()\n",
    "            if not re.search(rf\"(?i)\\b{canon}\\b\", target):\n",
    "                return f\"{target} {canon}\"\n",
    "    return txt   \n",
    "\n",
    "def _loose_contains(transcript: str, phrase: str) -> bool:\n",
    "    if not phrase:\n",
    "        return False\n",
    "\n",
    "    t_raw = (transcript or \"\").lower()\n",
    "    p_raw = (phrase or \"\").strip().lower().rstrip(\".\")\n",
    "    if p_raw and p_raw in t_raw:\n",
    "        return True\n",
    "\n",
    "    # canonical (strip lead verbs/determiners, normalize xN dayâ†’days)\n",
    "    t_can = _canonical(transcript)\n",
    "    p_can = _canonical(phrase)\n",
    "    if p_can and p_can in t_can:\n",
    "        return True\n",
    "\n",
    "    # day -> days tweak (raw)\n",
    "    p_alt = re.sub(r\"\\bx(\\d+)\\s*day\\b\", r\"x\\1 days\", p_raw)\n",
    "    if p_alt in t_raw:\n",
    "        return True\n",
    "\n",
    "    # NEW: modality order normalization check (e.g., 'X-ray ankle' <-> 'ankle X-ray')\n",
    "    try:\n",
    "        t_norm = _normalize_order_phrase(transcript).lower()\n",
    "        p_norm = _normalize_order_phrase(phrase).lower()\n",
    "        if p_norm and p_norm in t_norm:\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return False\n",
    "\n",
    "    \n",
    "\n",
    "def _ground_to_transcript(data: Dict[str, Any], transcript: str) -> Dict[str, Any]:\n",
    "    out = {}\n",
    "    for k in KEYS_ORDER:\n",
    "        v = data.get(k, [] if k in (\"diagnosis\",\"orders\",\"plan\") else \"\")\n",
    "        if isinstance(v, list):\n",
    "            kept, seen = [], set()\n",
    "            for s in v:\n",
    "                s2 = s.strip().rstrip(\".\")\n",
    "                key = _canonical(s2)\n",
    "                if s2 and key and key not in seen and _loose_contains(transcript, s2):\n",
    "                    seen.add(key); kept.append(s2)\n",
    "            out[k] = kept\n",
    "        else:\n",
    "            s2 = (v or \"\").strip().rstrip(\".\")\n",
    "            out[k] = s2 if s2 and _loose_contains(transcript, s2) else \"\"\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d744c23-4c6d-4a0e-8006-7e89263f03ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Few-shot-free field prompts (more specific & safer) ----------\n",
    "FIELD_PROMPTS = {\n",
    "    \"chief_complaint\": (\n",
    "        \"From the TRANSCRIPT, return the chief complaint as a verbatim substring.\\n\"\n",
    "        \"Return ONLY the phrase, with no quotes or extra words. If none, return nothing.\\n\\n\"\n",
    "        \"TRANSCRIPT:\\n{transcript}\\n\\nPHRASE:\"\n",
    "    ),\n",
    "    \"assessment\": (\n",
    "        \"From the TRANSCRIPT, return the assessment/impression as a verbatim substring \"\n",
    "        \"(e.g., 'Likely CAP', 'Likely uncomplicated UTI').\\n\"\n",
    "        \"Return ONLY the phrase, with no quotes or extra words. If none, return nothing.\\n\\n\"\n",
    "        \"TRANSCRIPT:\\n{transcript}\\n\\nPHRASE:\"\n",
    "    ),\n",
    "    \"follow_up\": (\n",
    "        \"From the TRANSCRIPT, return ONLY the follow-up timing as a verbatim substring \"\n",
    "        \"(e.g., '2 days', '1 week', 'return if worsening'). Do NOT include medications or 'PRN'.\\n\"\n",
    "        \"Return ONLY the phrase, with no quotes or extra words. If none, return nothing.\\n\\n\"\n",
    "        \"TRANSCRIPT:\\n{transcript}\\n\\nPHRASE:\"\n",
    "    ),\n",
    "    \"diagnosis\": (\n",
    "        \"From the TRANSCRIPT, list DIAGNOSES ONLY as a JSON array of verbatim substrings. \"\n",
    "        \"Strip hedges such as 'likely', 'suspected', 'possible', 'r/o'.\\n\"\n",
    "        \"Do NOT include orders, tests, medications, or follow-up instructions.\\n\"\n",
    "        'Return ONLY a JSON array, e.g. [\"community-acquired pneumonia\"]. If none, return [].\\n\\n'\n",
    "        \"TRANSCRIPT:\\n{transcript}\\n\\nARRAY:\"\n",
    "    ),\n",
    "    \"orders\": (\n",
    "        \"From the TRANSCRIPT, extract TESTS/PROCEDURES that are explicitly ordered \"\n",
    "        \"as a JSON array of minimal verbatim substrings (e.g., 'chest X-ray', 'ankle X-ray', 'urinalysis').\\n\"\n",
    "        \"Do NOT include medications or dosing (those belong in Plan). Do NOT include follow-up phrases.\\n\"\n",
    "        \"If multiple appear in a sentence, split into separate items.\\n\"\n",
    "        \"Return ONLY a JSON array. If none, return [].\\n\\n\"\n",
    "        \"TRANSCRIPT:\\n{transcript}\\n\\nARRAY:\"\n",
    "    ),\n",
    "    \"plan\": (\n",
    "        \"From the TRANSCRIPT, extract PLANNED INTERVENTIONS/INSTRUCTIONS as a JSON array of minimal verbatim substrings \"\n",
    "        \"(e.g., 'azithromycin 500 mg daily x5', 'ibuprofen 400 mg PRN', 'RICE', 'lifestyle modification').\\n\"\n",
    "        \"Include medications with dosing here. Do NOT include follow-up phrases or negative findings like 'No fever'.\\n\"\n",
    "        \"If multiple appear in a sentence, split into separate items.\\n\"\n",
    "        \"Return ONLY a JSON array. If none, return [].\\n\\n\"\n",
    "        \"TRANSCRIPT:\\n{transcript}\\n\\nARRAY:\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "def _gen_text(prompt: str) -> str:\n",
    "    \"\"\"Single-call generation wrapper.\"\"\"\n",
    "    return t5(prompt, **GEN_KW)[0][\"generated_text\"].strip()\n",
    "\n",
    "def _parse_array(s: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Best-effort array parser:\n",
    "    - Prefer proper JSON array if present.\n",
    "    - Else fall back to quoted-string extraction, else delimiter split.\n",
    "    - Final basic cleanup (strip/trim/unique happens later in refinement).\n",
    "    \"\"\"\n",
    "    s = (s or \"\").strip()\n",
    "\n",
    "    # Try to find the first JSON array region\n",
    "    m = re.search(r\"\\[[\\s\\S]*?\\]\", s)\n",
    "    if m:\n",
    "        block = m.group(0)\n",
    "        # Guard: if the block is absurdly long (e.g., copied transcript), skip JSON parse\n",
    "        if len(block) < 3000:\n",
    "            try:\n",
    "                arr = json.loads(block)\n",
    "                if isinstance(arr, list):\n",
    "                    return [x.strip() for x in arr if isinstance(x, str) and x.strip()]\n",
    "            except Exception:\n",
    "                pass  # fall through\n",
    "\n",
    "    # Next: look for quoted items\n",
    "    quoted = re.findall(r'\"([^\"]+)\"', s)\n",
    "    if quoted:\n",
    "        return [x.strip() for x in quoted if x and x.strip()]\n",
    "\n",
    "    # Fallback: split on commas/semicolons\n",
    "    rough = [x.strip() for x in re.split(r\"[;,]\", s) if x.strip()]\n",
    "    return rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "853b409f-6586-41bd-80ac-d7666f11e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_TIME_RE = re.compile(\n",
    "    r\"\\b(?:(?:in\\s+)?\\d+\\s*(?:day|days|week|weeks|wk|wks|month|months)|\"\n",
    "    r\"\\d+-\\d+\\s*(?:days|weeks)|\"\n",
    "    r\"(?:return if worse|return if worsening|follow up))\\b\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Replace your _split_conjunctions with this\n",
    "def _split_conjunctions(items: List[str], transcript: str) -> List[str]:\n",
    "    parts: List[str] = []\n",
    "    for it in items:\n",
    "        s = (it or \"\").strip().rstrip(\".\")\n",
    "        if not s:\n",
    "            continue\n",
    "        chunks = re.split(r\"\\b(?:and|then|,|;)\\b\", s, flags=re.IGNORECASE)\n",
    "        for c in chunks:\n",
    "            c2 = c.strip().strip(\",;.\").rstrip(\".\")\n",
    "            if not c2:\n",
    "                continue\n",
    "            # ðŸš« keep follow-up phrases OUT of arrays\n",
    "            low = c2.lower()\n",
    "            if low.startswith(\"follow up\") or _TIME_RE.search(c2):\n",
    "                continue\n",
    "            if _loose_contains(transcript, c2):\n",
    "                parts.append(c2)\n",
    "\n",
    "    seen, out = set(), []\n",
    "    for p in parts:\n",
    "        key = _canonical(p)\n",
    "        if key and key not in seen:\n",
    "            seen.add(key)\n",
    "            out.append(p)\n",
    "    return out or [x for x in items if x]\n",
    "\n",
    "    def _safe_split(s: str) -> List[str]:\n",
    "        s = s.strip().rstrip(\".\")\n",
    "        # quick guard: if the whole string is a dosage phrase, don't split it\n",
    "        if _DOSAGE_PHRASE_RE.search(s):\n",
    "            return [s]\n",
    "        # split on lightweight separators, but not in the middle of tokens\n",
    "        parts = re.split(r\"\\b(?:and|then)\\b|[;,]\", s, flags=re.IGNORECASE)\n",
    "        out = []\n",
    "        for p in parts:\n",
    "            p = re.sub(r\"^\\s*(?:to\\s+)\", \"\", p, flags=re.IGNORECASE).strip(\" ,;.\")\n",
    "            if p:\n",
    "                out.append(p)\n",
    "        return out\n",
    "\n",
    "    chunks = []\n",
    "    for it in items:\n",
    "        it = (it or \"\").strip()\n",
    "        if not it:\n",
    "            continue\n",
    "        chunks.extend(_safe_split(it))\n",
    "\n",
    "    # keep only those that are (loosely) present in transcript and dedup canonically\n",
    "    seen, kept = set(), []\n",
    "    for c in chunks:\n",
    "        key = _canonical(c)\n",
    "        if key and _loose_contains(transcript, c) and key not in seen:\n",
    "            seen.add(key)\n",
    "            kept.append(c)\n",
    "    return kept if kept else items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f9f32db-6e55-4f20-94f9-2695c8f1ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Follow-up detection (replace yours with this) ===\n",
    "_TIME_RE = re.compile(\n",
    "    r\"\\b(?:(?:in\\s+)?\\d+\\s*(?:day|days|week|weeks|wk|wks|month|months)|\"\n",
    "    r\"\\d+-\\d+\\s*(?:days|weeks)|\"\n",
    "    r\"(?:return if worse|return if worsening))\\b\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "\n",
    "def _extract_time_phrase(text: str) -> str:\n",
    "    \"\"\"Return a timing like '2 days', '1 week', or 'return if worsening'. Reject bare 'follow up'.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    m = _TIME_RE.search(text)\n",
    "    return (m.group(0).strip() if m else \"\").rstrip(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "363745a2-e806-4b77-8622-23e2951cd3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_DOSAGE_PHRASE_RE = re.compile(\n",
    "    r\"\\b([A-Za-z][A-Za-z\\-]*(?:\\s[A-Za-z][A-Za-z\\-]*)*\\s+\"            # drug name (words)\n",
    "    r\"(?:\\d+(?:\\.\\d+)?\\s*(?:mg|mcg|g|ml|mL|units)\\b\"                  # dose + unit\n",
    "    r\"(?:\\s*(?:daily|q\\d+h|bid|tid|qid|prn|BID|TID|QID|PRN))?\"        # frequency (optional)\n",
    "    r\"(?:\\s*x\\s*\\d+\\s*(?:day|days|week|weeks)?)?\"                     # duration (optional)\n",
    "    r\"))\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "def _extract_dosage_phrases(text: str) -> List[str]:\n",
    "    return [m.group(1).strip().rstrip(\".\") for m in _DOSAGE_PHRASE_RE.finditer(text or \"\")]\n",
    "\n",
    "_RULE_OUT_RE = re.compile(r\"([^\\.]*?)\\s+to\\s+rule\\s+out\\b\", re.IGNORECASE)\n",
    "def _extract_before_rule_out(text: str) -> List[str]:\n",
    "    out = []\n",
    "    for m in _RULE_OUT_RE.finditer(text or \"\"):\n",
    "        lhs = m.group(1).strip().rstrip(\".\")\n",
    "        chunks = re.split(r\"\\b(?:and|then|,|;)\\b\", lhs, flags=re.IGNORECASE)\n",
    "        out.extend([c.strip().strip(\",;.\") for c in chunks if c.strip()])\n",
    "    seen, dedup = set(), []\n",
    "    for x in out:\n",
    "        lx = _canonical(x)\n",
    "        if lx not in seen:\n",
    "            seen.add(lx); dedup.append(x)\n",
    "    return dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0994d9fb-257d-4f71-b9f1-d1f9ec30367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _post_clean_orders(items: List[str]) -> List[str]:\n",
    "    \"\"\"Keep Orders to tests/procedures only; strip meds/therapy artifacts.\"\"\"\n",
    "    cleaned = []\n",
    "    for s in items:\n",
    "        if not s:\n",
    "            continue\n",
    "        s2 = s.strip().rstrip(\".\")\n",
    "        # drop med/dosage phrases from Orders (they belong in Plan)\n",
    "        if _is_dosage_like(s2):\n",
    "            continue\n",
    "        # drop conjunction/verb artifacts like \"and start ...\"\n",
    "        if re.search(r\"^\\s*(and\\s+start|and\\s+begin)\\b\", s2, re.IGNORECASE):\n",
    "            continue\n",
    "        # drop obvious therapy words (go to Plan)\n",
    "        if re.search(r\"\\b(rice|rest|ice|compression|elevation|lifestyle|counseling)\\b\", s2, re.IGNORECASE):\n",
    "            continue\n",
    "        cleaned.append(s2)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def _first_imaging_or_lab_token(s: str) -> str:\n",
    "    \"\"\"Return the first imaging/lab token present, else ''.\"\"\"\n",
    "    w = (s or \"\").lower()\n",
    "    for tok in (HEURISTICS[\"imaging\"] | HEURISTICS[\"labs\"]):\n",
    "        if tok in w:\n",
    "            return tok\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def _split_mixed_test_and_med(s: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    If an item contains BOTH a test (e.g., 'urinalysis', 'x-ray') AND a dosage-like med,\n",
    "    split into ['<test>', '<med dosing>']. Otherwise return [s].\n",
    "    \"\"\"\n",
    "    s2 = (s or \"\").strip().rstrip(\".\")\n",
    "    if not s2:\n",
    "        return []\n",
    "\n",
    "    has_dose = _is_dosage_like(s2)\n",
    "    test_tok = _first_imaging_or_lab_token(s2)\n",
    "    if has_dose and test_tok:\n",
    "        # test fragment: keep the token minimally as the order (e.g., 'urinalysis', 'chest x-ray')\n",
    "        test_phrase = test_tok\n",
    "        # lift the first dosage phrase as the med\n",
    "        doses = _extract_dosage_phrases(s2)\n",
    "        med_phrase = doses[0] if doses else \"\"\n",
    "        out = []\n",
    "        if test_phrase:\n",
    "            # small normalization for x-ray side (e.g., 'ankle x-ray' / 'chest x-ray')\n",
    "            # if there is a body part, leave normalization to existing _normalize_order_phrase\n",
    "            out.append(test_phrase)\n",
    "        if med_phrase:\n",
    "            out.append(med_phrase)\n",
    "        return [x for x in out if x]\n",
    "    return [s2] if s2 else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75fa980b-316d-4da7-b9b2-9dec9be810a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_PATTERNS = [\n",
    "    (\"orders\", r\"\\b(order|obtain|get|perform|schedule)\\b\\s+([^\\.]+)\"),\n",
    "    (\"plan\",   r\"\\b(start|begin|initiate|recommend|advise|continue)\\b\\s+([^\\.]+)\"),\n",
    "]\n",
    "\n",
    "def _derive_actions_from_transcript(transcript: str) -> Dict[str, List[str]]:\n",
    "    text = re.sub(r\"\\s+\", \" \", transcript or \"\").strip()\n",
    "    derived = {\"orders\": [], \"plan\": []}\n",
    "\n",
    "    # 1) Verb-led extraction\n",
    "    for target, pat in ACTION_PATTERNS:\n",
    "        for m in re.finditer(pat, text, flags=re.IGNORECASE):\n",
    "            segment = m.group(2)\n",
    "            chunks = re.split(r\"\\b(?:and|then|,|;)\\b\", segment, flags=re.IGNORECASE)\n",
    "            for c in chunks:\n",
    "                c2 = re.sub(r\"^\\s*(to\\s+)\", \"\", c, flags=re.IGNORECASE).strip().rstrip(\".\")\n",
    "                if c2:\n",
    "                    derived[target].append(c2)\n",
    "\n",
    "    # 2) Dosage phrases â†’ Plan candidates\n",
    "    for phr in _extract_dosage_phrases(text):\n",
    "        derived[\"plan\"].append(phr)\n",
    "\n",
    "    # 3) Before \"to rule out ...\" â†’ Orders\n",
    "    for lhs in _extract_before_rule_out(text):\n",
    "        derived[\"orders\"].append(lhs)\n",
    "\n",
    "    # De-dup canonical\n",
    "    for k in derived:\n",
    "        seen, out = set(), []\n",
    "        for it in derived[k]:\n",
    "            key = _canonical(it)\n",
    "            if key and it and key not in seen:\n",
    "                seen.add(key)\n",
    "                out.append(it.strip())\n",
    "        derived[k] = out\n",
    "\n",
    "    # ðŸ”§ Post-clean Orders to remove dosage/verb artifacts\n",
    "    derived[\"orders\"] = _post_clean_orders(derived[\"orders\"])\n",
    "    return derived\n",
    "\n",
    "    # 5) Post-filters & normalization\n",
    "    #   - keep 'order/perform/...' phrases out of PLAN\n",
    "    derived[\"plan\"] = [p for p in derived[\"plan\"]\n",
    "                       if not re.search(r\"\\b(order|obtain|get|perform|schedule)\\b\", p, re.IGNORECASE)]\n",
    "    #   - normalize orders phrasing ('x-ray ankle' â†’ 'ankle X-ray', etc.)\n",
    "    derived[\"orders\"] = [_normalize_order_phrase(x) for x in derived[\"orders\"]]\n",
    "    #   - keep plan/orders items short unless dosage-like\n",
    "    derived[\"orders\"] = [x for x in derived[\"orders\"] if _keep_minimal(x)]\n",
    "    derived[\"plan\"]   = [x for x in derived[\"plan\"]   if _keep_minimal(x)]\n",
    "\n",
    "    return derived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35d7f71e-46b7-4559-9d7a-f33db5481c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORDER_VERBS = r\"(?:order|obtain|get|perform|schedule)\"\n",
    "PLAN_VERBS  = r\"(?:start|begin|initiate|recommend|advise|continue)\"\n",
    "\n",
    "def _clip_action_core(s: str, target: str) -> str:\n",
    "    txt = _LEAD_VERBS.sub(\"\", (s or \"\").strip().rstrip(\".\"))\n",
    "\n",
    "    # Try verb-specific capture\n",
    "    if target == \"orders\":\n",
    "        m = re.search(rf\"\\b{ORDER_VERBS}\\b\\s+(.*)$\", txt, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            txt = m.group(1).strip().rstrip(\".\")\n",
    "    elif target == \"plan\":\n",
    "        m = re.search(rf\"\\b{PLAN_VERBS}\\b\\s+(.*)$\", txt, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            txt = m.group(1).strip().rstrip(\".\")\n",
    "\n",
    "    # If there are multiple sentences, pick the first non follow-up, non-empty bit\n",
    "    candidates = [p.strip() for p in re.split(r\"[.]\", txt) if p.strip()]\n",
    "    for c in candidates:\n",
    "        if not (c.lower().startswith(\"follow up\") or _TIME_RE.search(c)):\n",
    "            return c\n",
    "    # Fallback\n",
    "    return candidates[0] if candidates else txt\n",
    "\n",
    "def _is_dosage_like(s: str) -> bool:\n",
    "    return bool(_DOSAGE_PHRASE_RE.search(s))\n",
    "\n",
    "def _keep_minimal(s: str) -> bool:\n",
    "    # keep any dosage; otherwise keep short, atomic phrases\n",
    "    if _is_dosage_like(s):\n",
    "        return True\n",
    "    s_ = s.strip().rstrip(\".\")\n",
    "    # drop rationales like \"to rule out fracture\" trailing bits\n",
    "    s_ = re.sub(r\"\\bto rule out\\b.*$\", \"\", s_, flags=re.IGNORECASE).strip()\n",
    "    # allow short tests/instructions\n",
    "    return len(s_.split()) <= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94e7f997-1018-47a3-ba42-c1648603e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEURISTICS = {\n",
    "    \"imaging\": {\"x-ray\", \"xray\", \"ct\", \"mri\", \"ultrasound\", \"ekg\", \"ecg\", \"echo\"},\n",
    "    \"labs\": {\"cbc\", \"cmp\", \"a1c\", \"bmp\", \"urinalysis\", \"culture\", \"strep test\"},\n",
    "}\n",
    "MIRROR_MEDS_TO_ORDERS = False  # demo: meds appear in Orders & Plan\n",
    "\n",
    "def _looks_like_imaging_or_lab(s: str) -> bool:\n",
    "    w = s.lower()\n",
    "    return any(tok in w for tok in (HEURISTICS[\"imaging\"] | HEURISTICS[\"labs\"]))\n",
    "\n",
    "def _route_items(orders: List[str], plan: List[str]) -> Tuple[List[str], List[str]]:\n",
    "    o2, p2 = [], []\n",
    "    for it in orders:\n",
    "        s = it.strip().rstrip(\".\")\n",
    "        if not s: continue\n",
    "        if _looks_like_imaging_or_lab(s):\n",
    "            o2.append(s)\n",
    "        elif _is_dosage_like(s):\n",
    "            if MIRROR_MEDS_TO_ORDERS: o2.append(s)\n",
    "            p2.append(s)\n",
    "        else:\n",
    "            o2.append(s)\n",
    "    for it in plan:\n",
    "        s = it.strip().rstrip(\".\")\n",
    "        if not s: continue\n",
    "        if _looks_like_imaging_or_lab(s):\n",
    "            o2.append(s)\n",
    "        elif _is_dosage_like(s):\n",
    "            if MIRROR_MEDS_TO_ORDERS: o2.append(s)\n",
    "            p2.append(s)\n",
    "        else:\n",
    "            p2.append(s)\n",
    "\n",
    "    def dedup(xs: List[str]) -> List[str]:\n",
    "        seen, out = set(), []\n",
    "        for x in xs:\n",
    "            key = _canonical(x)\n",
    "            if key and key not in seen:\n",
    "                seen.add(key); out.append(x.strip().rstrip(\".\"))\n",
    "        return out\n",
    "\n",
    "    return dedup(o2), dedup(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90473df7-2cbb-463a-88d4-c47e432dbe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _merge_unique(dst: List[str], src: List[str]) -> List[str]:\n",
    "    seen = {_canonical(x) for x in dst if x}\n",
    "    out = [d.strip().rstrip(\".\") for d in dst if d and _canonical(d)]\n",
    "    for s in src:\n",
    "        t = s.strip().rstrip(\".\")\n",
    "        key = _canonical(t)\n",
    "        if t and key and key not in seen:\n",
    "            seen.add(key)\n",
    "            out.append(t)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91715fc7-6869-4487-a0ef-ab979aa23cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _refine_empty_fields(transcript: str, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    filled = dict(data)\n",
    "\n",
    "    # ---- Strings (backoff if empty) ----\n",
    "    for k in [\"chief_complaint\", \"assessment\"]:\n",
    "        if not filled.get(k):\n",
    "            val = _gen_text(FIELD_PROMPTS[k].format(transcript=transcript)).strip()\n",
    "            filled[k] = val\n",
    "\n",
    "    # ---- Follow-up normalize (PRN not captured) ----\n",
    "    fu = filled.get(\"follow_up\", \"\")\n",
    "    if not fu:\n",
    "        fu = _gen_text(FIELD_PROMPTS[\"follow_up\"].format(transcript=transcript)).strip()\n",
    "    filled[\"follow_up\"] = _extract_time_phrase(fu)\n",
    "\n",
    "    # ---- Arrays ----\n",
    "    for k in [\"diagnosis\", \"orders\", \"plan\"]:\n",
    "        arr = filled.get(k, [])\n",
    "        if not arr:\n",
    "            raw = _gen_text(FIELD_PROMPTS[k].format(transcript=transcript))\n",
    "            arr = _parse_array(raw)\n",
    "\n",
    "        # split on and/then/,/; and drop follow-up-like fragments\n",
    "        arr = _split_conjunctions(arr, transcript)\n",
    "\n",
    "        # strip action verbs & normalize modality phrasing for orders/plan\n",
    "        if k in (\"orders\", \"plan\"):\n",
    "            arr = [_clip_action_core(x, k) for x in arr]\n",
    "            arr = [_normalize_order_phrase(x) for x in arr]\n",
    "\n",
    "        # orders-only: split mixed \"test + med\" items (e.g., \"urinalysis and nitrofurantoin...\")\n",
    "        if k == \"orders\":\n",
    "            arr = sum((_split_mixed_test_and_med(x) for x in arr), [])\n",
    "            # pre-clean orders to keep tests/procedures only (drop 'and start ...', dosages, etc.)\n",
    "            arr = _post_clean_orders(arr)\n",
    "\n",
    "        # de-hedge diagnosis labels\n",
    "        if k == \"diagnosis\":\n",
    "            arr = [_dehedge(x) for x in arr if _dehedge(x)]\n",
    "\n",
    "        # minimality, dedupe, and filter out follow-up phrases leaking into arrays\n",
    "        seen, clean = set(), []\n",
    "        for it in arr:\n",
    "            s = (it or \"\").strip().rstrip(\".\")\n",
    "            if not s:\n",
    "                continue\n",
    "            # keep follow-up out of arrays\n",
    "            if s.lower().startswith(\"follow up\") or _TIME_RE.search(s):\n",
    "                continue\n",
    "            if k in (\"orders\", \"plan\") and not _keep_minimal(s):\n",
    "                continue\n",
    "            key = _canonical(s)\n",
    "            if key and key not in seen:\n",
    "                seen.add(key)\n",
    "                clean.append(s)\n",
    "        filled[k] = clean\n",
    "\n",
    "    # safety pass de-hedge (in case any slipped through)\n",
    "    filled[\"diagnosis\"] = [_dehedge(x) for x in filled.get(\"diagnosis\", []) if _dehedge(x)]\n",
    "\n",
    "    # ---- Always derive & merge heuristic actions ----\n",
    "    derived = _derive_actions_from_transcript(transcript)\n",
    "    filled[\"orders\"] = _merge_unique(filled.get(\"orders\", []), derived.get(\"orders\", []))\n",
    "    filled[\"plan\"]   = _merge_unique(filled.get(\"plan\",   []), derived.get(\"plan\",   []))\n",
    "\n",
    "    # ðŸš¿ post-clean Orders: drop dosage-like and 'and start...' artifacts (second pass)\n",
    "    filled[\"orders\"] = _post_clean_orders(filled.get(\"orders\", []))\n",
    "\n",
    "    # salvage follow_up if still missing\n",
    "    if not filled.get(\"follow_up\"):\n",
    "        filled[\"follow_up\"] = _extract_time_phrase(transcript)\n",
    "\n",
    "    # ---- Prune Plan noise (drop sentences, hedges, and negatives like \"No fever\") ----\n",
    "    pruned_plan = []\n",
    "    for s in filled.get(\"plan\", []):\n",
    "        s2 = (s or \"\").strip()\n",
    "        if not s2:\n",
    "            continue\n",
    "        if s2.count(\".\") > 0:\n",
    "            continue\n",
    "        if re.search(r\"\\blikely\\b|\\border\\b\", s2, re.IGNORECASE):\n",
    "            continue\n",
    "        if re.search(r\"\\b(no |denies |without )\", s2, re.IGNORECASE):\n",
    "            continue\n",
    "        pruned_plan.append(s2)\n",
    "    filled[\"plan\"] = pruned_plan\n",
    "\n",
    "    # ---- Canonical routing (labs/imaging -> Orders; dosed meds -> Plan (+mirror if enabled)) ----\n",
    "    filled[\"orders\"], filled[\"plan\"] = _route_items(\n",
    "        filled.get(\"orders\", []), filled.get(\"plan\", [])\n",
    "    )\n",
    "\n",
    "    # ---- Final grounding to transcript ----\n",
    "    return _ground_to_transcript(filled, transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0827503d-6a96-4a70-b6ae-886e4f756ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _raw_output_is_bad_list(raw: str, transcript: str) -> bool:\n",
    "    s = (raw or \"\").strip()\n",
    "    if s.startswith(\"[\") and s.endswith(\"]\") and len(s) < 4000:\n",
    "        inner = re.sub(r'^\\[\\s*\"?|\\s*\"?\\]$', \"\", s).strip()\n",
    "        return len(inner) >= 20 and inner.lower() in (transcript or \"\").lower()\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76f07724-8ac0-4fee-be9b-a2d2cd1df6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Default stubs for missing globals (so import works cleanly) ----\n",
    "try:\n",
    "    GEN_KW\n",
    "except NameError:\n",
    "    GEN_KW = {\"max_new_tokens\": 512, \"temperature\": 0.0}\n",
    "\n",
    "try:\n",
    "    FEWSHOT\n",
    "except NameError:\n",
    "    FEWSHOT = \"{transcript}\"\n",
    "\n",
    "try:\n",
    "    ClinicalNote\n",
    "except NameError:\n",
    "    class ClinicalNote:\n",
    "        def __init__(self, **kwargs):\n",
    "            self.__dict__.update(kwargs)\n",
    "        def dict(self): return self.__dict__\n",
    "\n",
    "def extract_note(transcript: str, gen_kwargs: Dict[str, Any] = GEN_KW) -> Tuple[ClinicalNote, str]:\n",
    "    # Pass A â€” schema-only prompt\n",
    "    prompt = FEWSHOT.replace(\"{transcript}\", transcript.strip())\n",
    "    result = t5(prompt, **gen_kwargs)[0]\n",
    "    raw = result[\"generated_text\"]\n",
    "\n",
    "    # If raw is \"bad list\", force empty so backoff fully runs\n",
    "    if _raw_output_is_bad_list(raw, transcript):\n",
    "        data = {k: ([] if k in (\"diagnosis\",\"orders\",\"plan\") else \"\") for k in KEYS_ORDER}\n",
    "    else:\n",
    "        data = _ground_to_transcript(parse_fields_with_boundaries(raw), transcript)\n",
    "\n",
    "    # Pass B â€” refine + salvage + routing + grounding\n",
    "    data = _refine_empty_fields(transcript, data)\n",
    "\n",
    "    # Validate & clean\n",
    "    try:\n",
    "        note = ClinicalNote(**data)\n",
    "    except ValidationError:\n",
    "        note = ClinicalNote()\n",
    "    for k in [\"diagnosis\", \"orders\", \"plan\"]:\n",
    "        arr = getattr(note, k)\n",
    "        setattr(note, k, [x.strip() for x in arr if x and x.strip()])\n",
    "\n",
    "    return note, raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "023adb1e-0ff0-47a8-a494-39f0538e7456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Wrote /Users/saturnine/DocScribe/notebooks/src/extract_clinical.py\n",
      "âœ… Wrote /Users/saturnine/DocScribe/notebooks/src/compose_note.py\n"
     ]
    }
   ],
   "source": [
    "# === Write self-contained src/extract_clinical.py and src/compose_note.py ===\n",
    "from pathlib import Path\n",
    "import textwrap\n",
    "\n",
    "# Locate repo root (assumes this cell is run from notebooks/ or below)\n",
    "NB_DIR = Path.cwd()\n",
    "if (NB_DIR / \"src\").exists():\n",
    "    ROOT = NB_DIR\n",
    "else:\n",
    "    probe = NB_DIR\n",
    "    for _ in range(4):\n",
    "        if (probe / \"src\").exists():\n",
    "            break\n",
    "        probe = probe.parent\n",
    "    ROOT = probe\n",
    "\n",
    "SRC = ROOT / \"src\"\n",
    "SRC.mkdir(parents=True, exist_ok=True)\n",
    "(SRC / \"__init__.py\").write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "# ---------- extract_clinical.py (self-contained; no notebook globals required) ----------\n",
    "extract_code = textwrap.dedent(\"\"\"\n",
    "    # src/extract_clinical.py\n",
    "    import re, json, os\n",
    "    from typing import Dict, Any, List, Tuple\n",
    "\n",
    "    # -----------------------\n",
    "    # Model bootstrap (FLAN)\n",
    "    # -----------------------\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "    MODEL_NAME = os.environ.get(\"DOCSCRIBE_MODEL\", \"google/flan-t5-large\")\n",
    "    os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "    try:\n",
    "        import torch\n",
    "        DEVICE = 0 if torch.cuda.is_available() else -1\n",
    "    except Exception:\n",
    "        DEVICE = -1\n",
    "\n",
    "    GEN_KW = dict(\n",
    "        do_sample=False,\n",
    "        num_beams=4,\n",
    "        temperature=0.0,\n",
    "        max_new_tokens=420,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    # Load once when module is imported\n",
    "    _tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    _model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "    _t5 = pipeline(\"text2text-generation\", model=_model, tokenizer=_tokenizer, device=DEVICE)\n",
    "\n",
    "    # -----------------------\n",
    "    # Prompt (schema-only)\n",
    "    # -----------------------\n",
    "    FEWSHOT = \\\"\\\"\\\"You are a documentation assistant.\n",
    "\n",
    "    Return ONE valid JSON object ONLY. Start with '{' and end with '}'.\n",
    "    Use EXACTLY these keys and types:\n",
    "    - \"chief_complaint\": string\n",
    "    - \"assessment\": string\n",
    "    - \"diagnosis\": array of strings\n",
    "    - \"orders\": array of strings\n",
    "    - \"plan\": array of strings\n",
    "    - \"follow_up\": string\n",
    "\n",
    "    STRICT RULES:\n",
    "    - Derive content ONLY from the TRANSCRIPT text.\n",
    "    - Every value MUST be a verbatim substring of the TRANSCRIPT (case-insensitive allowed).\n",
    "    - If a value is not present, leave it \"\" (for strings) or [] (for arrays).\n",
    "    - Do NOT add any text before or after the JSON.\n",
    "\n",
    "    TRANSCRIPT:\n",
    "    {transcript}\n",
    "\n",
    "    JSON:\n",
    "    \\\"\\\"\\\"\n",
    "\n",
    "\n",
    "    # -----------------------\n",
    "    # Regex / helpers\n",
    "    # -----------------------\n",
    "    KEYS_ORDER = [\"chief_complaint\",\"assessment\",\"diagnosis\",\"orders\",\"plan\",\"follow_up\"]\n",
    "    KEYS_SET   = set(KEYS_ORDER)\n",
    "    KEY_START_RE = re.compile(r'(?:\"?(chief_complaint|assessment|diagnosis|orders|plan|follow_up)\"?\\\\s*:)', re.I)\n",
    "\n",
    "    _LEAD_VERBS = re.compile(r\"^\\\\s*(?:start|begin|initiate|recommend|advise|continue|order|obtain|get|perform|schedule)\\\\s+\", re.IGNORECASE)\n",
    "    _DETERMINERS = re.compile(r\"^\\\\s*(?:to|the|a|an)\\\\s+\", re.IGNORECASE)\n",
    "\n",
    "    _HEDGES = re.compile(\n",
    "        r\"\\\\b(likely|suspected|suspect|possible|probable|prob|consistent with|r/o|rule out)\\\\b[:\\\\s,-]*\",\n",
    "        re.IGNORECASE,\n",
    "    )\n",
    "\n",
    "    _TIME_RE = re.compile(\n",
    "        r\"\\\\b(?:(?:in\\\\s+)?\\\\d+\\\\s*(?:day|days|week|weeks|wk|wks|month|months)|\"\n",
    "        r\"\\\\d+-\\\\d+\\\\s*(?:days|weeks)|\"\n",
    "        r\"(?:return if worse|return if worsening|follow up))\\\\b\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    _DOSAGE_PHRASE_RE = re.compile(\n",
    "        r\"\\\\b([A-Za-z][A-Za-z\\\\-]*(?:\\\\s[A-Za-z][A-Za-z\\\\-]*)*\\\\s+\"\n",
    "        r\"(?:\\\\d+\\\\s*(?:mg|mcg|g|ml|units)\\\\b(?:\\\\s*(?:daily|q\\\\d+h|BID|TID|QID|PRN))?\"\n",
    "        r\"(?:\\\\s*x\\\\d+\\\\s*(?:day|days|week|weeks)?)?))\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    # modality canonicalization\n",
    "    _MODALITY_CANON = {\n",
    "        r\"(?:x[\\\\-\\\\s]?ray|xr|xray)\": \"X-ray\",\n",
    "        r\"(?:ct\\\\s*scan|ct)\": \"CT\",\n",
    "        r\"(?:mri)\": \"MRI\",\n",
    "        r\"(?:ultra\\\\s*sound|us)\": \"Ultrasound\",\n",
    "        r\"(?:ekg|ecg)\": \"ECG\",\n",
    "        r\"(?:echo|echocardiogram)\": \"Echo\",\n",
    "    }\n",
    "\n",
    "    ORDER_VERBS = r\"(?:order|obtain|get|perform|schedule)\"\n",
    "    PLAN_VERBS  = r\"(?:start|begin|initiate|recommend|advise|continue)\"\n",
    "\n",
    "    HEURISTICS = {\n",
    "        \"imaging\": {\"x-ray\", \"xray\", \"ct\", \"mri\", \"ultrasound\", \"ekg\", \"ecg\", \"echo\"},\n",
    "        \"labs\": {\"cbc\", \"cmp\", \"a1c\", \"bmp\", \"urinalysis\", \"culture\", \"strep test\"},\n",
    "    }\n",
    "    MIRROR_MEDS_TO_ORDERS = True  # demo behavior\n",
    "\n",
    "\n",
    "    # -----------------------\n",
    "    # Small utils\n",
    "    # -----------------------\n",
    "    def _gen_text(prompt: str) -> str:\n",
    "        return _t5(prompt, **GEN_KW)[0][\"generated_text\"].strip()\n",
    "\n",
    "    def _canonical(s: str) -> str:\n",
    "        if not s:\n",
    "            return \"\"\n",
    "        x = s.strip().rstrip(\".\")\n",
    "        x = _LEAD_VERBS.sub(\"\", x)\n",
    "        x = _DETERMINERS.sub(\"\", x)\n",
    "        x = re.sub(r\"\\\\s+\", \" \", x)\n",
    "        x = re.sub(r\"\\\\bx(\\\\d+)\\\\s*day\\\\b\", r\"x\\\\1 days\", x, flags=re.IGNORECASE)\n",
    "        return x.lower()\n",
    "\n",
    "    def _dehedge(s: str) -> str:\n",
    "        return _HEDGES.sub(\"\", s or \"\").strip(\" .,:;\")\n",
    "\n",
    "    def _loose_contains(transcript: str, phrase: str) -> bool:\n",
    "        if not phrase:\n",
    "            return False\n",
    "        t_raw = (transcript or \"\").lower()\n",
    "        p_raw = phrase.strip().lower().rstrip(\".\")\n",
    "        if p_raw and p_raw in t_raw:\n",
    "            return True\n",
    "        t_can = _canonical(transcript)\n",
    "        p_can = _canonical(phrase)\n",
    "        if p_can and p_can in t_can:\n",
    "            return True\n",
    "        p_alt = re.sub(r\"\\\\bx(\\\\d+)\\\\s*day\\\\b\", r\"x\\\\1 days\", p_raw)\n",
    "        return p_alt in t_raw\n",
    "\n",
    "    def _extract_time_phrase(text: str) -> str:\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        m = _TIME_RE.search(text)\n",
    "        return (m.group(0).strip() if m else \"\").rstrip(\".\")\n",
    "\n",
    "    def _extract_dosage_phrases(text: str) -> List[str]:\n",
    "        return [m.group(1).strip().rstrip(\".\") for m in _DOSAGE_PHRASE_RE.finditer(text or \"\")]\n",
    "\n",
    "    def _find_key_spans(txt: str) -> Dict[str, slice]:\n",
    "        spans, positions = {}, []\n",
    "        for m in KEY_START_RE.finditer(txt):\n",
    "            k = m.group(1).lower()\n",
    "            positions.append((k, m.start(), m.end()))\n",
    "        for i, (k, s, e) in enumerate(positions):\n",
    "            nxt = positions[i+1][1] if i+1 < len(positions) else len(txt)\n",
    "            spans[k] = slice(e, nxt)\n",
    "        return spans\n",
    "\n",
    "    def _grab_string_val(chunk: str) -> str:\n",
    "        m = re.search(r'\"\\\\s*([^\"]*?)\\\\s*\"', chunk)  # \"value\"\n",
    "        if m: return m.group(1).strip()\n",
    "        m = re.search(r':\\\\s*([^,\\\\]\\\\}]+)', chunk)    # : value\n",
    "        return m.group(1).strip() if m else \"\"\n",
    "\n",
    "    def _grab_list_val(chunk: str) -> List[str]:\n",
    "        m = re.search(r'\\\\[\\\\s*([^\\\\]]*?)\\\\s*\\\\]', chunk)\n",
    "        inside = m.group(1) if m else chunk\n",
    "        items = re.findall(r'\"([^\"]+)\"', inside) or [x.strip() for x in re.split(r'[;,]', inside) if x.strip()]\n",
    "        cleaned, seen = [], set()\n",
    "        for it in items:\n",
    "            it = it.strip()\n",
    "            if not it or it.lower() in KEYS_SET or len(it) <= 1:\n",
    "                continue\n",
    "            low = it.lower()\n",
    "            if low not in seen:\n",
    "                seen.add(low); cleaned.append(it)\n",
    "        return cleaned\n",
    "\n",
    "    def _parse_array(s: str) -> List[str]:\n",
    "        m = re.search(r\"\\\\[[\\\\s\\\\S]*\\\\]\", s)\n",
    "        if m:\n",
    "            try:\n",
    "                arr = json.loads(m.group(0))\n",
    "                if isinstance(arr, list):\n",
    "                    return [x for x in arr if isinstance(x, str)]\n",
    "            except Exception:\n",
    "                pass\n",
    "        items = re.findall(r'\"([^\"]+)\"', s)\n",
    "        return items or [x.strip() for x in re.split(r\"[;,]\", s) if x.strip()]\n",
    "\n",
    "    def parse_fields_with_boundaries(raw_txt: str) -> Dict[str, Any]:\n",
    "        t = (raw_txt or \"\").replace(\"â€œ\", '\"').replace(\"â€\", '\"').replace(\"â€™\", \"'\")\n",
    "        t = re.sub(r\"\\\\s+\", \" \", t).strip()\n",
    "\n",
    "        # Try JSON first\n",
    "        mjson = re.search(r\"\\\\{[\\\\s\\\\S]*\\\\}\", t)\n",
    "        if mjson:\n",
    "            block = mjson.group(0)\n",
    "            try:\n",
    "                data = json.loads(block)\n",
    "                data = {k: v for k, v in data.items() if k in KEYS_SET}\n",
    "                for k in KEYS_ORDER:\n",
    "                    data.setdefault(k, [] if k in (\"diagnosis\",\"orders\",\"plan\") else \"\")\n",
    "                return data\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # Boundary parse\n",
    "        spans = _find_key_spans(t)\n",
    "        data: Dict[str, Any] = {k: ([] if k in (\"diagnosis\",\"orders\",\"plan\") else \"\") for k in KEYS_ORDER}\n",
    "        for k in KEYS_ORDER:\n",
    "            if k not in spans:\n",
    "                continue\n",
    "            chunk = t[spans[k]]\n",
    "            data[k] = _grab_list_val(chunk) if k in (\"diagnosis\",\"orders\",\"plan\") else _grab_string_val(chunk)\n",
    "        return data\n",
    "\n",
    "    def _normalize_order_phrase(s: str) -> str:\n",
    "        \\\"\\\"\\\"Flip '<modality> <target>' â†’ '<target> <Modality>' and tidy spacing.\\\"\\\"\\\" \n",
    "        txt = re.sub(r\"\\\\s+\", \" \", s or \"\").strip().rstrip(\".\")\n",
    "        for mod_re, canon in _MODALITY_CANON.items():\n",
    "            m = re.match(rf\"(?i)^{mod_re}\\\\s+(.+)$\", txt)\n",
    "            if m:\n",
    "                target = m.group(1).strip()\n",
    "                if not re.search(rf\"(?i)\\\\b{canon}\\\\b\", target):\n",
    "                    return f\"{target} {canon}\".strip()\n",
    "        return txt\n",
    "\n",
    "    def _split_conjunctions(items: List[str], transcript: str) -> List[str]:\n",
    "        parts: List[str] = []\n",
    "        for it in items:\n",
    "            s = (it or \"\").strip().rstrip(\".\")\n",
    "            if not s:\n",
    "                continue\n",
    "            chunks = re.split(r\"\\\\b(?:and|then|,|;)\\\\b\", s, flags=re.IGNORECASE)\n",
    "            for c in chunks:\n",
    "                c2 = c.strip().strip(\",;.\").rstrip(\".\")\n",
    "                if not c2:\n",
    "                    continue\n",
    "                # keep follow-up phrases OUT of arrays\n",
    "                low = c2.lower()\n",
    "                if low.startswith(\"follow up\") or _TIME_RE.search(c2):\n",
    "                    continue\n",
    "                if _loose_contains(transcript, c2):\n",
    "                    parts.append(c2)\n",
    "\n",
    "        seen, out = set(), []\n",
    "        for p in parts:\n",
    "            key = _canonical(p)\n",
    "            if key and key not in seen:\n",
    "                seen.add(key)\n",
    "                out.append(p)\n",
    "        return out or [x for x in items if x]\n",
    "\n",
    "    def _clip_action_core(s: str, target: str) -> str:\n",
    "        txt = _LEAD_VERBS.sub(\"\", (s or \"\").strip().rstrip(\".\"))\n",
    "        if target == \"orders\":\n",
    "            m = re.search(rf\"\\\\b{ORDER_VERBS}\\\\b\\\\s+(.*)$\", txt, flags=re.IGNORECASE)\n",
    "            if m: return m.group(1).strip().rstrip(\".\")\n",
    "        elif target == \"plan\":\n",
    "            m = re.search(rf\"\\\\b{PLAN_VERBS}\\\\b\\\\s+(.*)$\", txt, flags=re.IGNORECASE)\n",
    "            if m: return m.group(1).strip().rstrip(\".\")\n",
    "        parts = [p.strip() for p in re.split(r\"[.]\", txt) if p.strip()]\n",
    "        return parts[-1] if parts else txt\n",
    "\n",
    "    def _is_dosage_like(s: str) -> bool:\n",
    "        return bool(_DOSAGE_PHRASE_RE.search(s or \"\"))\n",
    "\n",
    "    def _keep_minimal(s: str) -> bool:\n",
    "        n_words = len((s or \"\").split())\n",
    "        return n_words <= 12 or _is_dosage_like(s)\n",
    "\n",
    "    def _looks_like_imaging_or_lab(s: str) -> bool:\n",
    "        w = (s or \"\").lower()\n",
    "        return any(tok in w for tok in (HEURISTICS[\"imaging\"] | HEURISTICS[\"labs\"]))\n",
    "\n",
    "    def _route_items(orders: List[str], plan: List[str]) -> Tuple[List[str], List[str]]:\n",
    "        o2, p2 = [], []\n",
    "        for it in orders:\n",
    "            s = (it or \"\").strip().rstrip(\".\")\n",
    "            if not s: continue\n",
    "            if _looks_like_imaging_or_lab(s):\n",
    "                o2.append(s)\n",
    "            elif _is_dosage_like(s):\n",
    "                if MIRROR_MEDS_TO_ORDERS: o2.append(s)\n",
    "                p2.append(s)\n",
    "            else:\n",
    "                o2.append(s)\n",
    "        for it in plan:\n",
    "            s = (it or \"\").strip().rstrip(\".\")\n",
    "            if not s: continue\n",
    "            if _looks_like_imaging_or_lab(s):\n",
    "                o2.append(s)\n",
    "            elif _is_dosage_like(s):\n",
    "                if MIRROR_MEDS_TO_ORDERS: o2.append(s)\n",
    "                p2.append(s)\n",
    "            else:\n",
    "                p2.append(s)\n",
    "\n",
    "        def dedup(xs: List[str]) -> List[str]:\n",
    "            seen, out = set(), []\n",
    "            for x in xs:\n",
    "                key = _canonical(x)\n",
    "                if key and key not in seen:\n",
    "                    seen.add(key); out.append(x.strip().rstrip(\".\"))\n",
    "            return out\n",
    "\n",
    "        return dedup(o2), dedup(p2)\n",
    "\n",
    "    def _merge_unique(dst: List[str], src: List[str]) -> List[str]:\n",
    "        seen = {_canonical(x) for x in dst if x}\n",
    "        out = [d.strip().rstrip(\".\") for d in dst if d and _canonical(d)]\n",
    "        for s in src:\n",
    "            t = (s or \"\").strip().rstrip(\".\")\n",
    "            key = _canonical(t)\n",
    "            if t and key and key not in seen:\n",
    "                seen.add(key)\n",
    "                out.append(t)\n",
    "        return out\n",
    "\n",
    "    def _extract_before_rule_out(text: str) -> List[str]:\n",
    "        out = []\n",
    "        for m in re.finditer(r\"([^\\\\.]*?)\\\\s+to\\\\s+rule\\\\s+out\\\\b\", text or \"\", flags=re.IGNORECASE):\n",
    "            lhs = m.group(1).strip().rstrip(\".\")\n",
    "            chunks = re.split(r\"\\\\b(?:and|then|,|;)\\\\b\", lhs, flags=re.IGNORECASE)\n",
    "            out.extend([c.strip().strip(\",;.\") for c in chunks if c.strip()])\n",
    "        seen, dedup = set(), []\n",
    "        for x in out:\n",
    "            lx = _canonical(x)\n",
    "            if lx not in seen:\n",
    "                seen.add(lx); dedup.append(x)\n",
    "        return dedup\n",
    "\n",
    "    def _post_clean_orders(items: List[str]) -> List[str]:\n",
    "        \\\"\\\"\\\"Keep Orders to tests/procedures only; strip meds/therapy artifacts.\\\"\\\"\\\" \n",
    "        cleaned = []\n",
    "        for s in items:\n",
    "            if not s:\n",
    "                continue\n",
    "            s2 = s.strip().rstrip(\".\")\n",
    "            # drop med/dosage phrases from Orders (they belong in Plan)\n",
    "            if _is_dosage_like(s2):\n",
    "                continue\n",
    "            # drop conjunction/verb artifacts like \"and start ...\"\n",
    "            if re.search(r\"^\\\\s*(and\\\\s+start|and\\\\s+begin)\\\\b\", s2, re.IGNORECASE):\n",
    "                continue\n",
    "            # drop obvious therapy words (go to Plan)\n",
    "            if re.search(r\"\\\\b(rice|rest|ice|compression|elevation|lifestyle|counseling)\\\\b\", s2, re.IGNORECASE):\n",
    "                continue\n",
    "            cleaned.append(s2)\n",
    "        return cleaned\n",
    "\n",
    "    def _first_imaging_or_lab_token(s: str) -> str:\n",
    "        \\\"\\\"\\\"Return the first imaging/lab token present, else ''.\\\"\\\"\\\" \n",
    "        w = (s or \"\").lower()\n",
    "        for tok in (HEURISTICS[\"imaging\"] | HEURISTICS[\"labs\"]):\n",
    "            if tok in w:\n",
    "                return tok\n",
    "        return \"\"\n",
    "\n",
    "    def _split_mixed_test_and_med(s: str) -> List[str]:\n",
    "        \\\"\\\"\\\"If item has BOTH test (e.g., 'urinalysis', 'x-ray') AND a dosage-like med,\n",
    "        split into ['<test>', '<med dosing>']. Otherwise return [s].\\\"\\\"\\\" \n",
    "        s2 = (s or \"\").strip().rstrip(\".\")\n",
    "        if not s2:\n",
    "            return []\n",
    "        has_dose = _is_dosage_like(s2)\n",
    "        test_tok = _first_imaging_or_lab_token(s2)\n",
    "        if has_dose and test_tok:\n",
    "            test_phrase = test_tok\n",
    "            doses = _extract_dosage_phrases(s2)\n",
    "            med_phrase = doses[0] if doses else \"\"\n",
    "            out = []\n",
    "            if test_phrase:\n",
    "                out.append(test_phrase)\n",
    "            if med_phrase:\n",
    "                out.append(med_phrase)\n",
    "            return [x for x in out if x]\n",
    "        return [s2] if s2 else []\n",
    "\n",
    "    ACTION_PATTERNS = [\n",
    "        (\"orders\", rf\"\\\\b{ORDER_VERBS}\\\\b\\\\s+([^\\\\.]+)\"),\n",
    "        (\"plan\",   rf\"\\\\b{PLAN_VERBS}\\\\b\\\\s+([^\\\\.]+)\"),\n",
    "    ]\n",
    "\n",
    "    def _derive_actions_from_transcript(transcript: str) -> Dict[str, List[str]]:\n",
    "        text = re.sub(r\"\\\\s+\", \" \", transcript or \"\").strip()\n",
    "        derived = {\"orders\": [], \"plan\": []}\n",
    "\n",
    "        # 1) Verb-led extraction\n",
    "        for target, pat in ACTION_PATTERNS:\n",
    "            for m in re.finditer(pat, text, flags=re.IGNORECASE):\n",
    "                segment = m.group(1)\n",
    "                chunks = re.split(r\"\\\\b(?:and|then|,|;)\\\\b\", segment, flags=re.IGNORECASE)\n",
    "                for c in chunks:\n",
    "                    c2 = re.sub(r\"^\\\\s*(to\\\\s+)\", \"\", c, flags=re.IGNORECASE).strip().rstrip(\".\")\n",
    "                    if c2:\n",
    "                        derived[target].append(c2)\n",
    "\n",
    "        # 2) Dosage phrases â†’ Plan candidates\n",
    "        for phr in _extract_dosage_phrases(text):\n",
    "            derived[\"plan\"].append(phr)\n",
    "\n",
    "        # 3) Before \"to rule out ...\" â†’ Orders\n",
    "        for lhs in _extract_before_rule_out(text):\n",
    "            derived[\"orders\"].append(lhs)\n",
    "\n",
    "        # De-dup canonical\n",
    "        for k in derived:\n",
    "            seen, out = set(), []\n",
    "            for it in derived[k]:\n",
    "                key = _canonical(it)\n",
    "                if key and it:\n",
    "                    if key not in seen:\n",
    "                        seen.add(key); out.append(it.strip())\n",
    "            derived[k] = out\n",
    "\n",
    "        return derived\n",
    "\n",
    "    def _ground_to_transcript(data: Dict[str, Any], transcript: str) -> Dict[str, Any]:\n",
    "        out = {}\n",
    "        for k in KEYS_ORDER:\n",
    "            v = data.get(k, [] if k in (\"diagnosis\",\"orders\",\"plan\") else \"\")\n",
    "            if isinstance(v, list):\n",
    "                kept, seen = [], set()\n",
    "                for s in v:\n",
    "                    s2 = (s or \"\").strip().rstrip(\".\")\n",
    "                    key = _canonical(s2)\n",
    "                    if s2 and key and key not in seen and _loose_contains(transcript, s2):\n",
    "                        seen.add(key); kept.append(s2)\n",
    "                out[k] = kept\n",
    "            else:\n",
    "                s2 = (v or \"\").strip().rstrip(\".\")\n",
    "                out[k] = s2 if s2 and _loose_contains(transcript, s2) else \"\"\n",
    "        return out\n",
    "\n",
    "    def _raw_output_is_bad_list(raw: str, transcript: str) -> bool:\n",
    "        s = (raw or \"\").strip()\n",
    "        if s.startswith(\"[\") and s.endswith(\"]\") and len(s) < 4000:\n",
    "            inner = re.sub(r'^\\\\[\\\\s*\"?|\\\\s*\"?\\\\]$', \"\", s).strip()\n",
    "            return len(inner) >= 20 and inner.lower() in (transcript or \"\").lower()\n",
    "        return False\n",
    "\n",
    "    FIELD_PROMPTS = {\n",
    "        \"chief_complaint\": (\n",
    "            \"From the TRANSCRIPT, return the chief complaint as a verbatim substring.\\\\n\"\n",
    "            \"Return ONLY the phrase, no quotes, no extra text. If none, return nothing.\\\\n\\\\n\"\n",
    "            \"TRANSCRIPT:\\\\n{transcript}\\\\n\\\\nPHRASE:\"\n",
    "        ),\n",
    "        \"assessment\": (\n",
    "            \"From the TRANSCRIPT, return the assessment/impression as a verbatim substring.\\\\n\"\n",
    "            \"Return ONLY the phrase, no quotes, no extra text. If none, return nothing.\\\\n\\\\n\"\n",
    "            \"TRANSCRIPT:\\\\n{transcript}\\\\n\\\\nPHRASE:\"\n",
    "        ),\n",
    "        \"follow_up\": (\n",
    "            \"From the TRANSCRIPT, return ONLY the follow-up timing as a verbatim substring \"\n",
    "            \"(e.g., '2 days', '1 week', 'return if worsening'). Do not include medications or 'PRN'. \"\n",
    "            \"Return ONLY the phrase, no quotes, no extra text. If none, return nothing.\\\\n\\\\n\"\n",
    "            \"TRANSCRIPT:\\\\n{transcript}\\\\n\\\\nPHRASE:\"\n",
    "        ),\n",
    "        \"diagnosis\": (\n",
    "            \"From the TRANSCRIPT, list diagnoses as a JSON array of verbatim substrings.\\\\n\"\n",
    "            \"Return ONLY the JSON array (e.g., [\\\\\\\"...\\\\\\\"]). If none, return [].\\\\n\\\\n\"\n",
    "            \"TRANSCRIPT:\\\\n{transcript}\\\\n\\\\nARRAY:\"\n",
    "        ),\n",
    "        \"orders\": (\n",
    "            \"From the TRANSCRIPT, extract tests/procedures/medications that are explicitly ordered \"\n",
    "            \"as a JSON array of verbatim substrings (minimal phrases only, e.g., 'chest X-ray', \"\n",
    "            \"'azithromycin 500 mg daily x5'). If multiple are in one sentence, split into separate items. \"\n",
    "            \"Return ONLY the JSON array. If none, return [].\\\\n\\\\nTRANSCRIPT:\\\\n{transcript}\\\\n\\\\nARRAY:\"\n",
    "        ),\n",
    "        \"plan\": (\n",
    "            \"From the TRANSCRIPT, extract planned interventions/instructions as a JSON array of verbatim substrings \"\n",
    "            \"(minimal phrases only, e.g., 'RICE', 'ibuprofen 400 mg PRN'). If multiple are in one sentence, split into \"\n",
    "            \"separate items. Return ONLY the JSON array. If none, return [].\\\\n\\\\nTRANSCRIPT:\\\\n{transcript}\\\\n\\\\nARRAY:\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "    # -----------------------\n",
    "    # Refinement pipeline\n",
    "    # -----------------------\n",
    "    def _refine_empty_fields(transcript: str, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        filled = dict(data)\n",
    "\n",
    "        # Strings\n",
    "        for k in [\"chief_complaint\", \"assessment\"]:\n",
    "            if not filled.get(k):\n",
    "                val = _gen_text(FIELD_PROMPTS[k].format(transcript=transcript)).strip()\n",
    "                filled[k] = val\n",
    "\n",
    "        # Follow-up normalize (PRN not captured)\n",
    "        fu = filled.get(\"follow_up\", \"\")\n",
    "        if not fu:\n",
    "            fu = _gen_text(FIELD_PROMPTS[\"follow_up\"].format(transcript=transcript)).strip()\n",
    "        filled[\"follow_up\"] = _extract_time_phrase(fu)\n",
    "\n",
    "        # Arrays\n",
    "        for k in [\"diagnosis\", \"orders\", \"plan\"]:\n",
    "            arr = filled.get(k, [])\n",
    "            if not arr:\n",
    "                raw = _gen_text(FIELD_PROMPTS[k].format(transcript=transcript))\n",
    "                arr = _parse_array(raw)\n",
    "\n",
    "            # generic split (and/then/,/;)\n",
    "            arr = _split_conjunctions(arr, transcript)\n",
    "\n",
    "            # strip lead verbs & normalize modality phrasing for orders/plan\n",
    "            if k in (\"orders\", \"plan\"):\n",
    "                arr = [_clip_action_core(x, k) for x in arr]\n",
    "                arr = [_normalize_order_phrase(x) for x in arr]\n",
    "\n",
    "            # de-hedge diagnosis labels\n",
    "            if k == \"diagnosis\":\n",
    "                arr = [_dehedge(x) for x in arr if _dehedge(x)]\n",
    "\n",
    "            # de-dup & minimality\n",
    "            seen, clean = set(), []\n",
    "            for it in arr:\n",
    "                s = (it or \"\").strip().rstrip(\".\")\n",
    "                if not s:\n",
    "                    continue\n",
    "                if k in (\"orders\", \"plan\") and not _keep_minimal(s):\n",
    "                    continue\n",
    "                key = _canonical(s)\n",
    "                if key and key not in seen:\n",
    "                    seen.add(key)\n",
    "                    clean.append(s)\n",
    "            filled[k] = clean\n",
    "\n",
    "        # De-hedge again (safety)\n",
    "        filled[\"diagnosis\"] = [_dehedge(x) for x in filled.get(\"diagnosis\", []) if _dehedge(x)]\n",
    "\n",
    "        # Always derive & merge\n",
    "        derived = _derive_actions_from_transcript(transcript)\n",
    "        filled[\"orders\"] = _merge_unique(filled.get(\"orders\", []), derived.get(\"orders\", []))\n",
    "        filled[\"plan\"]   = _merge_unique(filled.get(\"plan\",   []), derived.get(\"plan\", []))\n",
    "\n",
    "        # Prune Plan noise\n",
    "        pruned_plan = []\n",
    "        for s in filled.get(\"plan\", []):\n",
    "            s2 = s.strip()\n",
    "            if s2.count(\".\") > 0:\n",
    "                continue\n",
    "            if re.search(r\"\\\\blikely\\\\b|\\\\border\\\\b\", s2, re.IGNORECASE):\n",
    "                continue\n",
    "            pruned_plan.append(s2)\n",
    "        filled[\"plan\"] = pruned_plan\n",
    "\n",
    "        # Canonical routing\n",
    "        filled[\"orders\"], filled[\"plan\"] = _route_items(\n",
    "            filled.get(\"orders\", []), filled.get(\"plan\", [])\n",
    "        )\n",
    "\n",
    "        # Ground to transcript\n",
    "        return _ground_to_transcript(filled, transcript)\n",
    "\n",
    "\n",
    "    # -----------------------\n",
    "    # Public entry point\n",
    "    # -----------------------\n",
    "    def extract_note(transcript: str, gen_kwargs: Dict[str, Any] = GEN_KW) -> Tuple[Dict[str, Any], str]:\n",
    "        \\\"\"\"\n",
    "        Returns (note_dict, raw_model_output)\n",
    "        note_dict has keys:\n",
    "          chief_complaint (str), assessment (str), diagnosis (list[str]),\n",
    "          orders (list[str]), plan (list[str]), follow_up (str)\n",
    "        \\\"\"\"\n",
    "        # Pass A â€” schema-only prompt\n",
    "        prompt = FEWSHOT.replace(\"{transcript}\", (transcript or \"\").strip())\n",
    "        result = _t5(prompt, **gen_kwargs)[0]\n",
    "        raw = result[\"generated_text\"]\n",
    "\n",
    "        # If raw is \"bad list\", force empty so backoff fully runs\n",
    "        if _raw_output_is_bad_list(raw, transcript):\n",
    "            data = {k: ([] if k in (\"diagnosis\",\"orders\",\"plan\") else \"\") for k in KEYS_ORDER}\n",
    "        else:\n",
    "            data = _ground_to_transcript(parse_fields_with_boundaries(raw), transcript)\n",
    "\n",
    "        # Pass B â€” refine + salvage + routing + grounding\n",
    "        filled = _refine_empty_fields(transcript, data)\n",
    "\n",
    "        # Final cleaning\n",
    "        out: Dict[str, Any] = {\n",
    "            \"chief_complaint\": filled.get(\"chief_complaint\", \"\").strip(),\n",
    "            \"assessment\": filled.get(\"assessment\", \"\").strip(),\n",
    "            \"diagnosis\": [x.strip() for x in filled.get(\"diagnosis\", []) if x and x.strip()],\n",
    "            \"orders\": [x.strip() for x in filled.get(\"orders\", []) if x and x.strip()],\n",
    "            \"plan\": [x.strip() for x in filled.get(\"plan\", []) if x and x.strip()],\n",
    "            \"follow_up\": filled.get(\"follow_up\", \"\").strip(),\n",
    "        }\n",
    "        return out, raw\n",
    "\"\"\").strip(\"\\n\")\n",
    "\n",
    "(SRC / \"extract_clinical.py\").write_text(extract_code, encoding=\"utf-8\")\n",
    "\n",
    "# ---------- compose_note.py ----------\n",
    "compose_code = textwrap.dedent(\"\"\"\n",
    "    # src/compose_note.py\n",
    "    from typing import Tuple, Dict, Any\n",
    "\n",
    "    def _to_dict(note) -> Dict[str, Any]:\n",
    "        if isinstance(note, dict):\n",
    "            return note\n",
    "        if hasattr(note, \"dict\"):\n",
    "            return note.dict()\n",
    "        fields = [\"chief_complaint\",\"assessment\",\"diagnosis\",\"orders\",\"plan\",\"follow_up\"]\n",
    "        return {k: getattr(note, k, \"\" if k in (\"chief_complaint\",\"assessment\",\"follow_up\") else []) for k in fields}\n",
    "\n",
    "    def compose_note(note) -> Tuple[str, str]:\n",
    "        data = _to_dict(note)\n",
    "        s = data.get(\"chief_complaint\") or \"â€”\"\n",
    "        o = \", \".join(data.get(\"orders\") or []) or \"â€”\"\n",
    "        a = data.get(\"assessment\") or (\", \".join(data.get(\"diagnosis\") or []) or \"â€”\")\n",
    "        p = \"; \".join(data.get(\"plan\") or []) or \"â€”\"\n",
    "        f = data.get(\"follow_up\") or \"â€”\"\n",
    "        soap = f\"S: {s}\\\\nO: {o}\\\\nA: {a}\\\\nP: {p}\\\\nFollow-up: {f}\"\n",
    "        summary = f\"Visit summary: {s}. Assessment: {a}. Plan: {p}. Follow-up: {f}.\"\n",
    "        return soap, summary\n",
    "\"\"\").strip(\"\\n\")\n",
    "\n",
    "(SRC / \"compose_note.py\").write_text(compose_code, encoding=\"utf-8\")\n",
    "\n",
    "print(\"âœ… Wrote\", SRC / \"extract_clinical.py\")\n",
    "print(\"âœ… Wrote\", SRC / \"compose_note.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9269d2f0-8e7b-4b3a-883b-67bd0f34d72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPLIT/CLIP/MIN: ['chest X-ray']\n",
      "DERIVED: {'orders': ['chest X-ray'], 'plan': ['azithromycin 500 mg daily x5', 'Order chest X-ray and start azithromycin 500 mg daily x5']}\n"
     ]
    }
   ],
   "source": [
    "_test = \"Order chest X-ray and start azithromycin 500 mg daily x5. Follow up in 2 days.\"\n",
    "arr = [_test]\n",
    "arr = _split_conjunctions(arr, _test)\n",
    "arr = [_clip_action_core(x, \"orders\") for x in arr]\n",
    "arr = [x for x in arr if _keep_minimal(x)]\n",
    "print(\"SPLIT/CLIP/MIN:\", arr)          # expect: ['chest X-ray', 'azithromycin 500 mg daily x5']\n",
    "\n",
    "derived = _derive_actions_from_transcript(_test)\n",
    "print(\"DERIVED:\", derived)             # orders includes 'chest X-ray'; plan includes the dosage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37856b03-d207-4c39-af01-9d1d377f1901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ©º DEMO 1\n",
      "TRANSCRIPT: Fever and cough for 3 days. Mild shortness of breath. Likely CAP. Order chest X-ray and start azithromycin 500 mg daily x5. Follow up in 2 days.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/docscribe/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â± Latency: 22.98 s\n",
      "\n",
      "ðŸ“‹ JSON:\n",
      " {\n",
      "  \"chief_complaint\": \"Fever and cough\",\n",
      "  \"assessment\": \"Likely CAP\",\n",
      "  \"diagnosis\": [],\n",
      "  \"orders\": [\n",
      "    \"chest X-ray\"\n",
      "  ],\n",
      "  \"plan\": [\n",
      "    \"azithromycin 500 mg daily x5\"\n",
      "  ],\n",
      "  \"follow_up\": \"2 days\"\n",
      "}\n",
      "\n",
      "ðŸ§¾ SOAP:\n",
      " S: Fever and cough\n",
      "O: chest X-ray\n",
      "A: Likely CAP\n",
      "P: azithromycin 500 mg daily x5\n",
      "Follow-up: 2 days\n",
      "\n",
      "\n",
      "=== RAW MODEL OUTPUT ===\n",
      " [\"fever and cough for 3 days. Mild shortness of breath. Likely CAP. Order chest X-ray and start azithromycin 500 mg daily x5. Follow up in 2 days.\"]\n",
      "================================================================================\n",
      "ðŸ©º DEMO 2\n",
      "TRANSCRIPT: Left ankle pain after inversion injury yesterday. Likely lateral ankle sprain. X-ray ankle to rule out fracture. RICE and ibuprofen 400 mg PRN.\n",
      "\n",
      "â± Latency: 25.72 s\n",
      "\n",
      "ðŸ“‹ JSON:\n",
      " {\n",
      "  \"chief_complaint\": \"left ankle pain\",\n",
      "  \"assessment\": \"Likely lateral ankle sprain\",\n",
      "  \"diagnosis\": [],\n",
      "  \"orders\": [\n",
      "    \"X-ray ankle\"\n",
      "  ],\n",
      "  \"plan\": [\n",
      "    \"RICE and ibuprofen 400 mg PRN\"\n",
      "  ],\n",
      "  \"follow_up\": \"\"\n",
      "}\n",
      "\n",
      "ðŸ§¾ SOAP:\n",
      " S: left ankle pain\n",
      "O: X-ray ankle\n",
      "A: Likely lateral ankle sprain\n",
      "P: RICE and ibuprofen 400 mg PRN\n",
      "Follow-up: â€”\n",
      "\n",
      "\n",
      "=== RAW MODEL OUTPUT ===\n",
      " [\"left ankle pain after inversion injury yesterday. Likely lateral ankle sprain. X-ray ankle to rule out fracture. RICE and ibuprofen 400 mg PRN.\"]\n",
      "================================================================================\n",
      "ðŸ©º DEMO 3\n",
      "TRANSCRIPT: Dysuria and urinary frequency for 2 days. No fever or flank pain. Likely uncomplicated UTI. Urinalysis and nitrofurantoin 100 mg BID x5 days.\n",
      "\n",
      "â± Latency: 20.65 s\n",
      "\n",
      "ðŸ“‹ JSON:\n",
      " {\n",
      "  \"chief_complaint\": \"Dysuria\",\n",
      "  \"assessment\": \"Uncomplicated UTI\",\n",
      "  \"diagnosis\": [],\n",
      "  \"orders\": [\n",
      "    \"Urinalysis and nitrofurantoin 100 mg BID x5 day\"\n",
      "  ],\n",
      "  \"plan\": [\n",
      "    \"nitrofurantoin 100 mg BID x5 days\"\n",
      "  ],\n",
      "  \"follow_up\": \"2 days\"\n",
      "}\n",
      "\n",
      "ðŸ§¾ SOAP:\n",
      " S: Dysuria\n",
      "O: Urinalysis and nitrofurantoin 100 mg BID x5 day\n",
      "A: Uncomplicated UTI\n",
      "P: nitrofurantoin 100 mg BID x5 days\n",
      "Follow-up: 2 days\n",
      "\n",
      "\n",
      "=== RAW MODEL OUTPUT ===\n",
      " [\"dysuria and urinary frequency for 2 days.\"]\n"
     ]
    }
   ],
   "source": [
    "demos = [\n",
    "    \"Fever and cough for 3 days. Mild shortness of breath. Likely CAP. \"\n",
    "    \"Order chest X-ray and start azithromycin 500 mg daily x5. Follow up in 2 days.\",\n",
    "\n",
    "    \"Left ankle pain after inversion injury yesterday. Likely lateral ankle sprain. \"\n",
    "    \"X-ray ankle to rule out fracture. RICE and ibuprofen 400 mg PRN.\",\n",
    "\n",
    "    \"Dysuria and urinary frequency for 2 days. No fever or flank pain. \"\n",
    "    \"Likely uncomplicated UTI. Urinalysis and nitrofurantoin 100 mg BID x5 days.\"\n",
    "]\n",
    "\n",
    "for i, demo in enumerate(demos, 1):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸ©º DEMO {i}\\nTRANSCRIPT:\", demo)\n",
    "    t0 = time.time()\n",
    "    note, raw = extract_note(demo)\n",
    "    dt = round(time.time()-t0, 2)\n",
    "\n",
    "    print(f\"\\nâ± Latency: {dt} s\")\n",
    "    print(\"\\nðŸ“‹ JSON:\\n\", note.pretty())\n",
    "    print(\"\\nðŸ§¾ SOAP:\\n\", compose_soap(note))\n",
    "    print(\"\\n=== RAW MODEL OUTPUT ===\\n\", raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c449085e-f03e-445a-af1c-094465b33902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================ FINAL RAW OUTPUT INSPECTION ================\n",
      "\n",
      "================================================================================\n",
      "ðŸ©º RAW OUTPUT TEST 1\n",
      "TRANSCRIPT: Fever and cough for 3 days. Mild shortness of breath. Likely CAP. Order chest X-ray and start azithromycin 500 mg daily x5. Follow up in 2 days.\n",
      "\n",
      "=== RAW MODEL OUTPUT ===\n",
      " [\"fever and cough for 3 days. Mild shortness of breath. Likely CAP. Order chest X-ray and start azithromycin 500 mg daily x5. Follow up in 2 days.\"]\n",
      "================================================================================\n",
      "ðŸ©º RAW OUTPUT TEST 2\n",
      "TRANSCRIPT: Left ankle pain after inversion injury yesterday. Likely lateral ankle sprain. X-ray ankle to rule out fracture. RICE and ibuprofen 400 mg PRN.\n",
      "\n",
      "=== RAW MODEL OUTPUT ===\n",
      " [\"left ankle pain after inversion injury yesterday. Likely lateral ankle sprain. X-ray ankle to rule out fracture. RICE and ibuprofen 400 mg PRN.\"]\n",
      "================================================================================\n",
      "ðŸ©º RAW OUTPUT TEST 3\n",
      "TRANSCRIPT: Dysuria and urinary frequency for 2 days. No fever or flank pain. Likely uncomplicated UTI. Urinalysis and nitrofurantoin 100 mg BID x5 days.\n",
      "\n",
      "=== RAW MODEL OUTPUT ===\n",
      " [\"dysuria and urinary frequency for 2 days.\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n================ FINAL RAW OUTPUT INSPECTION ================\\n\")\n",
    "for i, txt in enumerate(demos, 1):\n",
    "    prompt = FEWSHOT.replace(\"{transcript}\", txt.strip())\n",
    "    raw_out = t5(prompt, **GEN_KW)[0][\"generated_text\"]\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸ©º RAW OUTPUT TEST {i}\")\n",
    "    print(\"TRANSCRIPT:\", txt)\n",
    "    print(\"\\n=== RAW MODEL OUTPUT ===\\n\", raw_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bd8bfed-5791-4837-927c-b408bee3fcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import OK. Example SOAP:\n",
      " S: Fever and cough\n",
      "O: â€”\n",
      "A: Fever and cough\n",
      "P: â€”\n",
      "Follow-up: follow up\n"
     ]
    }
   ],
   "source": [
    "# This tests that your files can be imported *from disk*\n",
    "import importlib, sys\n",
    "for mod in [\"src.extract_clinical\", \"src.compose_note\"]:\n",
    "    if mod in sys.modules: del sys.modules[mod]\n",
    "\n",
    "ec = importlib.import_module(\"src.extract_clinical\")\n",
    "cn = importlib.import_module(\"src.compose_note\")\n",
    "\n",
    "assert hasattr(ec, \"extract_note\"), \"extract_note is missing in src/extract_clinical.py\"\n",
    "assert hasattr(cn, \"compose_note\"), \"compose_note is missing in src/compose_note.py\"\n",
    "\n",
    "note, raw = ec.extract_note(\"Fever and cough for 3 days. Follow up in 2 days.\")\n",
    "soap, summary = cn.compose_note(note)\n",
    "print(\"Import OK. Example SOAP:\\n\", soap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b879ed66-6f6e-43f4-80b5-977cf877335f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DocScribe",
   "language": "python",
   "name": "docscribe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
