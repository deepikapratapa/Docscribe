{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9aec9d5-23e8-40e9-ba26-c39f14c52d47",
   "metadata": {},
   "source": [
    "# üß† DocScribe ‚Äî 02 ¬∑ Extractor Development (Public, No Hardcoding)\n",
    "\n",
    "- Model: `google/flan-t5-large` (public)\n",
    "- Prompt: schema-only (no clinical examples)\n",
    "- Parsing: per-key boundary parsing + grounding (verbatim / relaxed match)\n",
    "- Backoff: per-field micro-prompts (extractive) + minimal generic regex salvage\n",
    "- Routing: imaging/labs ‚Üí Orders; dosed meds ‚Üí Plan (mirrored to Orders for demo)\n",
    "- Fixes: verb stripping for both targets, PRN not treated as follow-up, robust split with relaxed containment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4cfa035-e399-4f17-a3c0-a588f547daa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /Users/saturnine/DocScribe\n",
      "SRC : /Users/saturnine/DocScribe/src\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# If this notebook lives in <repo>/notebooks/, ROOT is the repo root.\n",
    "NB_DIR = Path.cwd()\n",
    "if NB_DIR.name.lower() != \"notebooks\":\n",
    "    # fallback: look upward for a \"notebooks\" folder\n",
    "    probe = NB_DIR\n",
    "    for _ in range(4):\n",
    "        if (probe / \"notebooks\").exists():\n",
    "            break\n",
    "        probe = probe.parent\n",
    "    ROOT = probe\n",
    "else:\n",
    "    ROOT = NB_DIR.parent\n",
    "\n",
    "SRC = ROOT / \"src\"\n",
    "SRC.mkdir(exist_ok=True)\n",
    "\n",
    "# Make 'src' importable in *this* kernel too (not required for 03, but handy)\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"SRC :\", SRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e95bf1-75b2-4333-a280-cbaf9f1b9fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Device: CPU\n",
      "üß© Model: google/flan-t5-large\n",
      "üîÑ Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/docscribe/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model ready.\n"
     ]
    }
   ],
   "source": [
    "import os, re, json, time, torch\n",
    "from typing import Dict, Any, List, Tuple\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "torch.manual_seed(42)\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "DEVICE = 0 if torch.cuda.is_available() else -1\n",
    "MODEL_NAME = os.environ.get(\"DOCSCRIBE_MODEL\", \"google/flan-t5-large\")\n",
    "\n",
    "print(\"‚úÖ Device:\", \"GPU\" if DEVICE >= 0 else \"CPU\")\n",
    "print(\"üß© Model:\", MODEL_NAME)\n",
    "\n",
    "# Deterministic gen; for faster CPU demos set num_beams=1, max_new_tokens=320\n",
    "GEN_KW = dict(\n",
    "    do_sample=False,\n",
    "    num_beams=4,\n",
    "    temperature=0.0,\n",
    "    max_new_tokens=420,\n",
    "    early_stopping=True,\n",
    ")\n",
    "\n",
    "print(\"üîÑ Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "t5 = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=DEVICE)\n",
    "print(\"‚úÖ Model ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da2f5ef2-4e3b-4053-ad62-ab5c5bdb36a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClinicalNote(BaseModel):\n",
    "    chief_complaint: str = Field(default=\"\")\n",
    "    assessment: str = Field(default=\"\")\n",
    "    diagnosis: List[str] = Field(default_factory=list)\n",
    "    orders: List[str] = Field(default_factory=list)\n",
    "    plan: List[str] = Field(default_factory=list)\n",
    "    follow_up: str = Field(default=\"\")\n",
    "\n",
    "    def pretty(self) -> str:\n",
    "        return self.json(indent=2, ensure_ascii=False, exclude_none=True)\n",
    "\n",
    "def compose_soap(note: ClinicalNote) -> str:\n",
    "    s = note.chief_complaint or \"‚Äî\"\n",
    "    o = \", \".join(note.orders) if note.orders else \"‚Äî\"\n",
    "    a = note.assessment or (\", \".join(note.diagnosis) if note.diagnosis else \"‚Äî\")\n",
    "    p = \"; \".join(note.plan) if note.plan else \"‚Äî\"\n",
    "    f = note.follow_up or \"‚Äî\"\n",
    "    return f\"S: {s}\\nO: {o}\\nA: {a}\\nP: {p}\\nFollow-up: {f}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "066e6875-2f24-4e0e-bd0f-0114f8fad2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Prompt ready.\n"
     ]
    }
   ],
   "source": [
    "FEWSHOT = \"\"\"You are a documentation assistant.\n",
    "\n",
    "Return ONE valid JSON object ONLY. Start with '{' and end with '}'.\n",
    "Use EXACTLY these keys and types:\n",
    "- \"chief_complaint\": string\n",
    "- \"assessment\": string\n",
    "- \"diagnosis\": array of strings\n",
    "- \"orders\": array of strings\n",
    "- \"plan\": array of strings\n",
    "- \"follow_up\": string\n",
    "\n",
    "STRICT RULES:\n",
    "- Derive content ONLY from the TRANSCRIPT text.\n",
    "- Every value MUST be a verbatim substring of the TRANSCRIPT (case-insensitive allowed).\n",
    "- If a value is not present, leave it \"\" (for strings) or [] (for arrays).\n",
    "- Do NOT add any text before or after the JSON.\n",
    "\n",
    "TRANSCRIPT:\n",
    "{transcript}\n",
    "\n",
    "JSON:\n",
    "\"\"\"\n",
    "print(\"üìã Prompt ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e349c6d-eeb0-4558-abed-68f8c2634e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYS_ORDER = [\"chief_complaint\",\"assessment\",\"diagnosis\",\"orders\",\"plan\",\"follow_up\"]\n",
    "KEYS_SET   = set(KEYS_ORDER)\n",
    "KEY_START_RE = re.compile(r'(?:\"?(chief_complaint|assessment|diagnosis|orders|plan|follow_up)\"?\\s*:)', re.I)\n",
    "\n",
    "def _find_key_spans(txt: str) -> Dict[str, slice]:\n",
    "    spans, positions = {}, []\n",
    "    for m in KEY_START_RE.finditer(txt):\n",
    "        k = m.group(1).lower()\n",
    "        positions.append((k, m.start(), m.end()))\n",
    "    for i, (k, s, e) in enumerate(positions):\n",
    "        nxt = positions[i+1][1] if i+1 < len(positions) else len(txt)\n",
    "        spans[k] = slice(e, nxt)\n",
    "    return spans\n",
    "\n",
    "def _grab_string_val(chunk: str) -> str:\n",
    "    m = re.search(r'\"\\s*([^\"]*?)\\s*\"', chunk)  # \"value\"\n",
    "    if m: return m.group(1).strip()\n",
    "    m = re.search(r':\\s*([^,\\]\\}]+)', chunk)    # : value\n",
    "    return m.group(1).strip() if m else \"\"\n",
    "\n",
    "def _grab_list_val(chunk: str) -> List[str]:\n",
    "    m = re.search(r'\\[\\s*([^\\]]*?)\\s*\\]', chunk)\n",
    "    inside = m.group(1) if m else chunk\n",
    "    items = re.findall(r'\"([^\"]+)\"', inside) or [x.strip() for x in re.split(r'[;,]', inside) if x.strip()]\n",
    "    cleaned, seen = [], set()\n",
    "    for it in items:\n",
    "        it = it.strip()\n",
    "        if not it or it.lower() in KEYS_SET or len(it) <= 1:\n",
    "            continue\n",
    "        low = it.lower()\n",
    "        if low not in seen:\n",
    "            seen.add(low); cleaned.append(it)\n",
    "    return cleaned\n",
    "\n",
    "def parse_fields_with_boundaries(raw_txt: str) -> Dict[str, Any]:\n",
    "    t = (raw_txt or \"\").replace(\"‚Äú\", '\"').replace(\"‚Äù\", '\"').replace(\"‚Äô\", \"'\")\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "    # Try JSON first\n",
    "    mjson = re.search(r\"\\{[\\s\\S]*\\}\", t)\n",
    "    if mjson:\n",
    "        block = mjson.group(0)\n",
    "        try:\n",
    "            data = json.loads(block)\n",
    "            data = {k: v for k, v in data.items() if k in KEYS_SET}\n",
    "            for k in KEYS_ORDER:\n",
    "                data.setdefault(k, [] if k in (\"diagnosis\",\"orders\",\"plan\") else \"\")\n",
    "            return data\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Boundary parse\n",
    "    spans = _find_key_spans(t)\n",
    "    data: Dict[str, Any] = {k: ([] if k in (\"diagnosis\",\"orders\",\"plan\") else \"\") for k in KEYS_ORDER}\n",
    "    for k in KEYS_ORDER:\n",
    "        if k not in spans:\n",
    "            continue\n",
    "        chunk = t[spans[k]]\n",
    "        data[k] = _grab_list_val(chunk) if k in (\"diagnosis\",\"orders\",\"plan\") else _grab_string_val(chunk)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae266536-b96d-499e-8de5-e004efb9c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_LEAD_VERBS = re.compile(r\"^\\s*(?:start|begin|initiate|recommend|advise|continue|order|obtain|get|perform|schedule)\\s+\", re.IGNORECASE)\n",
    "_DETERMINERS = re.compile(r\"^\\s*(?:to|the|a|an)\\s+\", re.IGNORECASE)\n",
    "\n",
    "def _canonical(s: str) -> str:\n",
    "    if not s: \n",
    "        return \"\"\n",
    "    x = s.strip().rstrip(\".\")\n",
    "    x = _LEAD_VERBS.sub(\"\", x)\n",
    "    x = _DETERMINERS.sub(\"\", x)\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "    x = re.sub(r\"\\bx(\\d+)\\s*day\\b\", r\"x\\1 days\", x, flags=re.IGNORECASE)  # day‚Üídays\n",
    "    return x.lower()\n",
    "\n",
    "def _loose_contains(transcript: str, phrase: str) -> bool:\n",
    "    if not phrase:\n",
    "        return False\n",
    "    t_raw = (transcript or \"\").lower()\n",
    "    p_raw = phrase.strip().lower().rstrip(\".\")\n",
    "    if p_raw and p_raw in t_raw:\n",
    "        return True\n",
    "    # canonical forms\n",
    "    t_can = _canonical(transcript)\n",
    "    p_can = _canonical(phrase)\n",
    "    if p_can and p_can in t_can:\n",
    "        return True\n",
    "    # day -> days tweak\n",
    "    p_alt = re.sub(r\"\\bx(\\d+)\\s*day\\b\", r\"x\\1 days\", p_raw)\n",
    "    return p_alt in t_raw\n",
    "\n",
    "def _ground_to_transcript(data: Dict[str, Any], transcript: str) -> Dict[str, Any]:\n",
    "    out = {}\n",
    "    for k in KEYS_ORDER:\n",
    "        v = data.get(k, [] if k in (\"diagnosis\",\"orders\",\"plan\") else \"\")\n",
    "        if isinstance(v, list):\n",
    "            kept, seen = [], set()\n",
    "            for s in v:\n",
    "                s2 = s.strip().rstrip(\".\")\n",
    "                key = _canonical(s2)\n",
    "                if s2 and key and key not in seen and _loose_contains(transcript, s2):\n",
    "                    seen.add(key); kept.append(s2)\n",
    "            out[k] = kept\n",
    "        else:\n",
    "            s2 = (v or \"\").strip().rstrip(\".\")\n",
    "            out[k] = s2 if s2 and _loose_contains(transcript, s2) else \"\"\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d744c23-4c6d-4a0e-8006-7e89263f03ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIELD_PROMPTS = {\n",
    "    \"chief_complaint\": (\n",
    "        \"From the TRANSCRIPT, return the chief complaint as a verbatim substring.\\n\"\n",
    "        \"Return ONLY the phrase, no quotes, no extra text. If none, return nothing.\\n\\n\"\n",
    "        \"TRANSCRIPT:\\n{transcript}\\n\\nPHRASE:\"\n",
    "    ),\n",
    "    \"assessment\": (\n",
    "        \"From the TRANSCRIPT, return the assessment/impression as a verbatim substring.\\n\"\n",
    "        \"Return ONLY the phrase, no quotes, no extra text. If none, return nothing.\\n\\n\"\n",
    "        \"TRANSCRIPT:\\n{transcript}\\n\\nPHRASE:\"\n",
    "    ),\n",
    "    \"follow_up\": (\n",
    "        \"From the TRANSCRIPT, return ONLY the follow-up timing as a verbatim substring \"\n",
    "        \"(e.g., '2 days', '1 week', 'return if worsening'). Do not include medications or 'PRN'. \"\n",
    "        \"Return ONLY the phrase, no quotes, no extra text. If none, return nothing.\\n\\n\"\n",
    "        \"TRANSCRIPT:\\n{transcript}\\n\\nPHRASE:\"\n",
    "    ),\n",
    "    \"diagnosis\": (\n",
    "        \"From the TRANSCRIPT, list diagnoses as a JSON array of verbatim substrings.\\n\"\n",
    "        \"Return ONLY the JSON array (e.g., [\\\"...\\\"]). If none, return [].\\n\\n\"\n",
    "        \"TRANSCRIPT:\\n{transcript}\\n\\nARRAY:\"\n",
    "    ),\n",
    "    \"orders\": (\n",
    "        \"From the TRANSCRIPT, extract tests/procedures/medications that are explicitly ordered \"\n",
    "        \"as a JSON array of verbatim substrings (minimal phrases only, e.g., 'chest X-ray', \"\n",
    "        \"'azithromycin 500 mg daily x5'). If multiple are in one sentence, split into separate items. \"\n",
    "        \"Return ONLY the JSON array. If none, return [].\\n\\nTRANSCRIPT:\\n{transcript}\\n\\nARRAY:\"\n",
    "    ),\n",
    "    \"plan\": (\n",
    "        \"From the TRANSCRIPT, extract planned interventions/instructions as a JSON array of verbatim substrings \"\n",
    "        \"(minimal phrases only, e.g., 'RICE', 'ibuprofen 400 mg PRN'). If multiple are in one sentence, split into \"\n",
    "        \"separate items. Return ONLY the JSON array. If none, return [].\\n\\nTRANSCRIPT:\\n{transcript}\\n\\nARRAY:\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "def _gen_text(prompt: str) -> str:\n",
    "    return t5(prompt, **GEN_KW)[0][\"generated_text\"].strip()\n",
    "\n",
    "def _parse_array(s: str) -> List[str]:\n",
    "    m = re.search(r\"\\[[\\s\\S]*\\]\", s)\n",
    "    if m:\n",
    "        try:\n",
    "            arr = json.loads(m.group(0))\n",
    "            if isinstance(arr, list):\n",
    "                return [x for x in arr if isinstance(x, str)]\n",
    "        except Exception:\n",
    "            pass\n",
    "    items = re.findall(r'\"([^\"]+)\"', s)\n",
    "    return items or [x.strip() for x in re.split(r\"[;,]\", s) if x.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "853b409f-6586-41bd-80ac-d7666f11e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_conjunctions(items: List[str], transcript: str) -> List[str]:\n",
    "    parts: List[str] = []\n",
    "    for it in items:\n",
    "        s = it.strip().rstrip(\".\")\n",
    "        chunks = re.split(r\"\\b(?:and|then|,|;)\\b\", s, flags=re.IGNORECASE)\n",
    "        for c in chunks:\n",
    "            c2 = c.strip().strip(\",;.\")\n",
    "            if c2 and _loose_contains(transcript, c2):\n",
    "                parts.append(c2)\n",
    "    seen, out = set(), []\n",
    "    for p in parts:\n",
    "        key = _canonical(p)\n",
    "        if key and key not in seen:\n",
    "            seen.add(key)\n",
    "            out.append(p.strip())\n",
    "    return out or items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f9f32db-6e55-4f20-94f9-2695c8f1ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "_TIME_RE = re.compile(\n",
    "    r\"\\b(?:(?:in\\s+)?\\d+\\s*(?:day|days|week|weeks|wk|wks|month|months)|\"\n",
    "    r\"\\d+-\\d+\\s*(?:days|weeks)|\"\n",
    "    r\"(?:return if worse|return if worsening))\\b\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "def _extract_time_phrase(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    m = _TIME_RE.search(text)\n",
    "    return (m.group(0).strip() if m else \"\").rstrip(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "363745a2-e806-4b77-8622-23e2951cd3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_DOSAGE_PHRASE_RE = re.compile(\n",
    "    r\"\\b([A-Za-z][A-Za-z\\-]*(?:\\s[A-Za-z][A-Za-z\\-]*)*\\s+\"\n",
    "    r\"(?:\\d+\\s*(?:mg|mcg|g|ml|units)\\b(?:\\s*(?:daily|q\\d+h|BID|TID|QID|PRN))?\"\n",
    "    r\"(?:\\s*x\\d+\\s*(?:day|days|week|weeks)?)?))\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "def _extract_dosage_phrases(text: str) -> List[str]:\n",
    "    return [m.group(1).strip().rstrip(\".\") for m in _DOSAGE_PHRASE_RE.finditer(text or \"\")]\n",
    "\n",
    "_RULE_OUT_RE = re.compile(r\"([^\\.]*?)\\s+to\\s+rule\\s+out\\b\", re.IGNORECASE)\n",
    "def _extract_before_rule_out(text: str) -> List[str]:\n",
    "    out = []\n",
    "    for m in _RULE_OUT_RE.finditer(text or \"\"):\n",
    "        lhs = m.group(1).strip().rstrip(\".\")\n",
    "        chunks = re.split(r\"\\b(?:and|then|,|;)\\b\", lhs, flags=re.IGNORECASE)\n",
    "        out.extend([c.strip().strip(\",;.\") for c in chunks if c.strip()])\n",
    "    seen, dedup = set(), []\n",
    "    for x in out:\n",
    "        lx = _canonical(x)\n",
    "        if lx not in seen:\n",
    "            seen.add(lx); dedup.append(x)\n",
    "    return dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75fa980b-316d-4da7-b9b2-9dec9be810a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_PATTERNS = [\n",
    "    (\"orders\", r\"\\b(order|obtain|get|perform|schedule)\\b\\s+([^\\.]+)\"),\n",
    "    (\"plan\",   r\"\\b(start|begin|initiate|recommend|advise|continue)\\b\\s+([^\\.]+)\"),\n",
    "]\n",
    "def _derive_actions_from_transcript(transcript: str) -> Dict[str, List[str]]:\n",
    "    text = re.sub(r\"\\s+\", \" \", transcript or \"\").strip()\n",
    "    derived = {\"orders\": [], \"plan\": []}\n",
    "\n",
    "    # 1) Verb-led extraction\n",
    "    for target, pat in ACTION_PATTERNS:\n",
    "        for m in re.finditer(pat, text, flags=re.IGNORECASE):\n",
    "            segment = m.group(2)\n",
    "            chunks = re.split(r\"\\b(?:and|then|,|;)\\b\", segment, flags=re.IGNORECASE)\n",
    "            for c in chunks:\n",
    "                c2 = re.sub(r\"^\\s*(to\\s+)\", \"\", c, flags=re.IGNORECASE).strip().rstrip(\".\")\n",
    "                if c2:\n",
    "                    derived[target].append(c2)\n",
    "\n",
    "    # 2) Dosage phrases ‚Üí Plan candidates\n",
    "    for phr in _extract_dosage_phrases(text):\n",
    "        derived[\"plan\"].append(phr)\n",
    "\n",
    "    # 3) Before \"to rule out ...\" ‚Üí Orders\n",
    "    for lhs in _extract_before_rule_out(text):\n",
    "        derived[\"orders\"].append(lhs)\n",
    "\n",
    "    # De-dup canonical\n",
    "    for k in derived:\n",
    "        seen, out = set(), []\n",
    "        for it in derived[k]:\n",
    "            key = _canonical(it)\n",
    "            if key and it:\n",
    "                if key not in seen:\n",
    "                    seen.add(key); out.append(it.strip())\n",
    "        derived[k] = out\n",
    "    return derived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35d7f71e-46b7-4559-9d7a-f33db5481c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORDER_VERBS = r\"(?:order|obtain|get|perform|schedule)\"\n",
    "PLAN_VERBS  = r\"(?:start|begin|initiate|recommend|advise|continue)\"\n",
    "\n",
    "def _clip_action_core(s: str, target: str) -> str:\n",
    "    # NEW: strip any action verb first (applies to both targets)\n",
    "    txt = _LEAD_VERBS.sub(\"\", s.strip().rstrip(\".\"))  # <‚Äî key change\n",
    "    if target == \"orders\":\n",
    "        m = re.search(rf\"\\b{ORDER_VERBS}\\b\\s+(.*)$\", txt, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            return m.group(1).strip().rstrip(\".\")\n",
    "    elif target == \"plan\":\n",
    "        m = re.search(rf\"\\b{PLAN_VERBS}\\b\\s+(.*)$\", txt, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            return m.group(1).strip().rstrip(\".\")\n",
    "    parts = [p.strip() for p in re.split(r\"[.]\", txt) if p.strip()]\n",
    "    return parts[-1] if parts else txt\n",
    "\n",
    "def _is_dosage_like(s: str) -> bool:\n",
    "    return bool(_DOSAGE_PHRASE_RE.search(s))\n",
    "\n",
    "def _keep_minimal(s: str) -> bool:\n",
    "    n_words = len(s.split())\n",
    "    return n_words <= 12 or _is_dosage_like(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94e7f997-1018-47a3-ba42-c1648603e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEURISTICS = {\n",
    "    \"imaging\": {\"x-ray\", \"xray\", \"ct\", \"mri\", \"ultrasound\", \"ekg\", \"ecg\", \"echo\"},\n",
    "    \"labs\": {\"cbc\", \"cmp\", \"a1c\", \"bmp\", \"urinalysis\", \"culture\", \"strep test\"},\n",
    "}\n",
    "MIRROR_MEDS_TO_ORDERS = True  # demo: meds appear in Orders & Plan\n",
    "\n",
    "def _looks_like_imaging_or_lab(s: str) -> bool:\n",
    "    w = s.lower()\n",
    "    return any(tok in w for tok in (HEURISTICS[\"imaging\"] | HEURISTICS[\"labs\"]))\n",
    "\n",
    "def _route_items(orders: List[str], plan: List[str]) -> Tuple[List[str], List[str]]:\n",
    "    o2, p2 = [], []\n",
    "    for it in orders:\n",
    "        s = it.strip().rstrip(\".\")\n",
    "        if not s: continue\n",
    "        if _looks_like_imaging_or_lab(s):\n",
    "            o2.append(s)\n",
    "        elif _is_dosage_like(s):\n",
    "            if MIRROR_MEDS_TO_ORDERS: o2.append(s)\n",
    "            p2.append(s)\n",
    "        else:\n",
    "            o2.append(s)\n",
    "    for it in plan:\n",
    "        s = it.strip().rstrip(\".\")\n",
    "        if not s: continue\n",
    "        if _looks_like_imaging_or_lab(s):\n",
    "            o2.append(s)\n",
    "        elif _is_dosage_like(s):\n",
    "            if MIRROR_MEDS_TO_ORDERS: o2.append(s)\n",
    "            p2.append(s)\n",
    "        else:\n",
    "            p2.append(s)\n",
    "\n",
    "    def dedup(xs: List[str]) -> List[str]:\n",
    "        seen, out = set(), []\n",
    "        for x in xs:\n",
    "            key = _canonical(x)\n",
    "            if key and key not in seen:\n",
    "                seen.add(key); out.append(x.strip().rstrip(\".\"))\n",
    "        return out\n",
    "\n",
    "    return dedup(o2), dedup(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90473df7-2cbb-463a-88d4-c47e432dbe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _merge_unique(dst: List[str], src: List[str]) -> List[str]:\n",
    "    seen = {_canonical(x) for x in dst if x}\n",
    "    out = [d.strip().rstrip(\".\") for d in dst if d and _canonical(d)]\n",
    "    for s in src:\n",
    "        t = s.strip().rstrip(\".\")\n",
    "        key = _canonical(t)\n",
    "        if t and key and key not in seen:\n",
    "            seen.add(key)\n",
    "            out.append(t)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91715fc7-6869-4487-a0ef-ab979aa23cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _refine_empty_fields(transcript: str, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    filled = dict(data)\n",
    "\n",
    "    # Strings\n",
    "    for k in [\"chief_complaint\", \"assessment\"]:\n",
    "        if not filled.get(k):\n",
    "            val = _gen_text(FIELD_PROMPTS[k].format(transcript=transcript)).strip()\n",
    "            filled[k] = val\n",
    "\n",
    "    # Follow-up normalize (PRN not captured)\n",
    "    fu = filled.get(\"follow_up\", \"\")\n",
    "    if not fu:\n",
    "        fu = _gen_text(FIELD_PROMPTS[\"follow_up\"].format(transcript=transcript)).strip()\n",
    "    filled[\"follow_up\"] = _extract_time_phrase(fu)\n",
    "\n",
    "    # Arrays\n",
    "    for k in [\"diagnosis\", \"orders\", \"plan\"]:\n",
    "        arr = filled.get(k, [])\n",
    "        if not arr:\n",
    "            raw = _gen_text(FIELD_PROMPTS[k].format(transcript=transcript))\n",
    "            arr = _parse_array(raw)\n",
    "\n",
    "        arr = _split_conjunctions(arr, transcript)\n",
    "\n",
    "        if k in (\"orders\", \"plan\"):\n",
    "            arr = [_clip_action_core(x, k) for x in arr]\n",
    "\n",
    "        seen, clean = set(), []\n",
    "        for it in arr:\n",
    "            s = it.strip().rstrip(\".\")\n",
    "            if not s:\n",
    "                continue\n",
    "            if k in (\"orders\",\"plan\") and not _keep_minimal(s):\n",
    "                continue\n",
    "            key = _canonical(s)\n",
    "            if key and key not in seen:\n",
    "                seen.add(key); clean.append(s)\n",
    "        filled[k] = clean\n",
    "\n",
    "    # Always derive & merge\n",
    "    derived = _derive_actions_from_transcript(transcript)\n",
    "    filled[\"orders\"] = _merge_unique(filled.get(\"orders\", []), derived.get(\"orders\", []))\n",
    "    filled[\"plan\"]   = _merge_unique(filled.get(\"plan\",   []), derived.get(\"plan\",   []))\n",
    "\n",
    "    # Prune Plan noise\n",
    "    pruned_plan = []\n",
    "    for s in filled.get(\"plan\", []):\n",
    "        s2 = s.strip()\n",
    "        if s2.count(\".\") > 0:\n",
    "            continue\n",
    "        if re.search(r\"\\blikely\\b|\\border\\b\", s2, re.IGNORECASE):\n",
    "            continue\n",
    "        pruned_plan.append(s2)\n",
    "    filled[\"plan\"] = pruned_plan\n",
    "\n",
    "    # Canonical routing\n",
    "    filled[\"orders\"], filled[\"plan\"] = _route_items(filled.get(\"orders\", []), filled.get(\"plan\", []))\n",
    "\n",
    "    # Ground to transcript\n",
    "    return _ground_to_transcript(filled, transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0827503d-6a96-4a70-b6ae-886e4f756ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _raw_output_is_bad_list(raw: str, transcript: str) -> bool:\n",
    "    s = (raw or \"\").strip()\n",
    "    if s.startswith(\"[\") and s.endswith(\"]\") and len(s) < 4000:\n",
    "        inner = re.sub(r'^\\[\\s*\"?|\\s*\"?\\]$', \"\", s).strip()\n",
    "        return len(inner) >= 20 and inner.lower() in (transcript or \"\").lower()\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76f07724-8ac0-4fee-be9b-a2d2cd1df6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_note(transcript: str, gen_kwargs: Dict[str, Any] = GEN_KW) -> Tuple[ClinicalNote, str]:\n",
    "    # Pass A ‚Äî schema-only prompt\n",
    "    prompt = FEWSHOT.replace(\"{transcript}\", transcript.strip())\n",
    "    result = t5(prompt, **gen_kwargs)[0]\n",
    "    raw = result[\"generated_text\"]\n",
    "\n",
    "    # If raw is \"bad list\", force empty so backoff fully runs\n",
    "    if _raw_output_is_bad_list(raw, transcript):\n",
    "        data = {k: ([] if k in (\"diagnosis\",\"orders\",\"plan\") else \"\") for k in KEYS_ORDER}\n",
    "    else:\n",
    "        data = _ground_to_transcript(parse_fields_with_boundaries(raw), transcript)\n",
    "\n",
    "    # Pass B ‚Äî refine + salvage + routing + grounding\n",
    "    data = _refine_empty_fields(transcript, data)\n",
    "\n",
    "    # Validate & clean\n",
    "    try:\n",
    "        note = ClinicalNote(**data)\n",
    "    except ValidationError:\n",
    "        note = ClinicalNote()\n",
    "    for k in [\"diagnosis\", \"orders\", \"plan\"]:\n",
    "        arr = getattr(note, k)\n",
    "        setattr(note, k, [x.strip() for x in arr if x and x.strip()])\n",
    "\n",
    "    return note, raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "023adb1e-0ff0-47a8-a494-39f0538e7456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Wrote src/extract_clinical.py and src/compose_note.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import textwrap\n",
    "\n",
    "# 1) Make src a package\n",
    "(SRC / \"__init__.py\").write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "# 2) Write extract_clinical.py (PUT YOUR REAL IMPLEMENTATION HERE)\n",
    "#    - Replace the placeholder '... your code here ...' with your working extractor\n",
    "extract_code = textwrap.dedent(\"\"\"\n",
    "    import re, json\n",
    "    from typing import Dict, Any, List, Tuple\n",
    "\n",
    "    # -------------------------\n",
    "    # Your real extractor pieces\n",
    "    # -------------------------\n",
    "    # - model/pipeline init (FLAN / Clinical-T5 fallback / etc.)\n",
    "    # - coerce_json, parse helpers\n",
    "    # - extract_note(text) -> (note_dict, raw_model_output)\n",
    "    # Make sure extract_note RETURNS a dict with keys:\n",
    "    #  chief_complaint, assessment, diagnosis(list), orders(list), plan(list), follow_up\n",
    "    #\n",
    "    # Below is a very small placeholder you should REPLACE with your actual extractor from 02.\n",
    "\n",
    "    def extract_note(transcript: str) -> Tuple[Dict[str, Any], str]:\n",
    "        t = (transcript or \"\").strip()\n",
    "        # TODO: replace this block with your *real* model-backed extractor\n",
    "        note = {\n",
    "            \"chief_complaint\": \"\",\n",
    "            \"assessment\": \"\",\n",
    "            \"diagnosis\": [],\n",
    "            \"orders\": [],\n",
    "            \"plan\": [],\n",
    "            \"follow_up\": \"\"\n",
    "        }\n",
    "        raw = t\n",
    "        return note, raw\n",
    "\"\"\").strip() + \"\\n\"\n",
    "(SRC / \"extract_clinical.py\").write_text(extract_code, encoding=\"utf-8\")\n",
    "\n",
    "# 3) Write compose_note.py (use your working composer)\n",
    "compose_code = textwrap.dedent(\"\"\"\n",
    "    from typing import Tuple, Dict, Any\n",
    "\n",
    "    def _to_dict(note) -> Dict[str, Any]:\n",
    "        if isinstance(note, dict):\n",
    "            return note\n",
    "        if hasattr(note, \"dict\"):\n",
    "            return note.dict()\n",
    "        fields = [\"chief_complaint\",\"assessment\",\"diagnosis\",\"orders\",\"plan\",\"follow_up\"]\n",
    "        return {k: getattr(note, k, \"\" if k in (\"chief_complaint\",\"assessment\",\"follow_up\") else []) for k in fields}\n",
    "\n",
    "    def compose_note(note) -> Tuple[str, str]:\n",
    "        data = _to_dict(note)\n",
    "        s = data.get(\"chief_complaint\") or \"‚Äî\"\n",
    "        o = \", \".join(data.get(\"orders\") or []) or \"‚Äî\"\n",
    "        a = data.get(\"assessment\") or (\", \".join(data.get(\"diagnosis\") or []) or \"‚Äî\")\n",
    "        p = \"; \".join(data.get(\"plan\") or []) or \"‚Äî\"\n",
    "        f = data.get(\"follow_up\") or \"‚Äî\"\n",
    "        soap = f\"S: {s}\\\\nO: {o}\\\\nA: {a}\\\\nP: {p}\\\\nFollow-up: {f}\"\n",
    "        summary = f\"Visit summary: {s}. Assessment: {a}. Plan: {p}. Follow-up: {f}.\"\n",
    "        return soap, summary\n",
    "\"\"\").strip() + \"\\n\"\n",
    "(SRC / \"compose_note.py\").write_text(compose_code, encoding=\"utf-8\")\n",
    "\n",
    "print(\"‚úÖ Wrote src/extract_clinical.py and src/compose_note.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9269d2f0-8e7b-4b3a-883b-67bd0f34d72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPLIT/CLIP/MIN: ['chest X-ray', 'Follow up in 2 days']\n",
      "DERIVED: {'orders': ['chest X-ray', 'start azithromycin 500 mg daily x5'], 'plan': ['azithromycin 500 mg daily x5', 'Order chest X-ray and start azithromycin 500 mg daily x5']}\n"
     ]
    }
   ],
   "source": [
    "_test = \"Order chest X-ray and start azithromycin 500 mg daily x5. Follow up in 2 days.\"\n",
    "arr = [_test]\n",
    "arr = _split_conjunctions(arr, _test)\n",
    "arr = [_clip_action_core(x, \"orders\") for x in arr]\n",
    "arr = [x for x in arr if _keep_minimal(x)]\n",
    "print(\"SPLIT/CLIP/MIN:\", arr)          # expect: ['chest X-ray', 'azithromycin 500 mg daily x5']\n",
    "\n",
    "derived = _derive_actions_from_transcript(_test)\n",
    "print(\"DERIVED:\", derived)             # orders includes 'chest X-ray'; plan includes the dosage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37856b03-d207-4c39-af01-9d1d377f1901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ü©∫ DEMO 1\n",
      "TRANSCRIPT: Fever and cough for 3 days. Mild shortness of breath. Likely CAP. Order chest X-ray and start azithromycin 500 mg daily x5. Follow up in 2 days.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/docscribe/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è± Latency: 30.08 s\n",
      "\n",
      "üìã JSON:\n",
      " {\n",
      "  \"chief_complaint\": \"Fever and cough\",\n",
      "  \"assessment\": \"Likely CAP\",\n",
      "  \"diagnosis\": [],\n",
      "  \"orders\": [\n",
      "    \"chest X-ray\",\n",
      "    \"start azithromycin 500 mg daily x5\"\n",
      "  ],\n",
      "  \"plan\": [\n",
      "    \"start azithromycin 500 mg daily x5\"\n",
      "  ],\n",
      "  \"follow_up\": \"2 days\"\n",
      "}\n",
      "\n",
      "üßæ SOAP:\n",
      " S: Fever and cough\n",
      "O: chest X-ray, start azithromycin 500 mg daily x5\n",
      "A: Likely CAP\n",
      "P: start azithromycin 500 mg daily x5\n",
      "Follow-up: 2 days\n",
      "\n",
      "\n",
      "=== RAW MODEL OUTPUT ===\n",
      " [\"fever and cough for 3 days. Mild shortness of breath. Likely CAP. Order chest X-ray and start azithromycin 500 mg daily x5. Follow up in 2 days.\"]\n",
      "================================================================================\n",
      "ü©∫ DEMO 2\n",
      "TRANSCRIPT: Left ankle pain after inversion injury yesterday. Likely lateral ankle sprain. X-ray ankle to rule out fracture. RICE and ibuprofen 400 mg PRN.\n",
      "\n",
      "‚è± Latency: 26.7 s\n",
      "\n",
      "üìã JSON:\n",
      " {\n",
      "  \"chief_complaint\": \"left ankle pain\",\n",
      "  \"assessment\": \"Likely lateral ankle sprain\",\n",
      "  \"diagnosis\": [],\n",
      "  \"orders\": [\n",
      "    \"X-ray ankle\",\n",
      "    \"RICE and ibuprofen 400 mg PRN\"\n",
      "  ],\n",
      "  \"plan\": [\n",
      "    \"RICE and ibuprofen 400 mg PRN\"\n",
      "  ],\n",
      "  \"follow_up\": \"\"\n",
      "}\n",
      "\n",
      "üßæ SOAP:\n",
      " S: left ankle pain\n",
      "O: X-ray ankle, RICE and ibuprofen 400 mg PRN\n",
      "A: Likely lateral ankle sprain\n",
      "P: RICE and ibuprofen 400 mg PRN\n",
      "Follow-up: ‚Äî\n",
      "\n",
      "\n",
      "=== RAW MODEL OUTPUT ===\n",
      " [\"left ankle pain after inversion injury yesterday. Likely lateral ankle sprain. X-ray ankle to rule out fracture. RICE and ibuprofen 400 mg PRN.\"]\n",
      "================================================================================\n",
      "ü©∫ DEMO 3\n",
      "TRANSCRIPT: Dysuria and urinary frequency for 2 days. No fever or flank pain. Likely uncomplicated UTI. Urinalysis and nitrofurantoin 100 mg BID x5 days.\n",
      "\n",
      "‚è± Latency: 21.73 s\n",
      "\n",
      "üìã JSON:\n",
      " {\n",
      "  \"chief_complaint\": \"Dysuria\",\n",
      "  \"assessment\": \"Likely uncomplicated UTI\",\n",
      "  \"diagnosis\": [],\n",
      "  \"orders\": [\n",
      "    \"Urinalysis\",\n",
      "    \"Urinalysis and nitrofurantoin 100 mg BID x5 day\"\n",
      "  ],\n",
      "  \"plan\": [],\n",
      "  \"follow_up\": \"2 days\"\n",
      "}\n",
      "\n",
      "üßæ SOAP:\n",
      " S: Dysuria\n",
      "O: Urinalysis, Urinalysis and nitrofurantoin 100 mg BID x5 day\n",
      "A: Likely uncomplicated UTI\n",
      "P: ‚Äî\n",
      "Follow-up: 2 days\n",
      "\n",
      "\n",
      "=== RAW MODEL OUTPUT ===\n",
      " [\"dysuria and urinary frequency for 2 days.\"]\n"
     ]
    }
   ],
   "source": [
    "demos = [\n",
    "    \"Fever and cough for 3 days. Mild shortness of breath. Likely CAP. \"\n",
    "    \"Order chest X-ray and start azithromycin 500 mg daily x5. Follow up in 2 days.\",\n",
    "\n",
    "    \"Left ankle pain after inversion injury yesterday. Likely lateral ankle sprain. \"\n",
    "    \"X-ray ankle to rule out fracture. RICE and ibuprofen 400 mg PRN.\",\n",
    "\n",
    "    \"Dysuria and urinary frequency for 2 days. No fever or flank pain. \"\n",
    "    \"Likely uncomplicated UTI. Urinalysis and nitrofurantoin 100 mg BID x5 days.\"\n",
    "]\n",
    "\n",
    "for i, demo in enumerate(demos, 1):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ü©∫ DEMO {i}\\nTRANSCRIPT:\", demo)\n",
    "    t0 = time.time()\n",
    "    note, raw = extract_note(demo)\n",
    "    dt = round(time.time()-t0, 2)\n",
    "\n",
    "    print(f\"\\n‚è± Latency: {dt} s\")\n",
    "    print(\"\\nüìã JSON:\\n\", note.pretty())\n",
    "    print(\"\\nüßæ SOAP:\\n\", compose_soap(note))\n",
    "    print(\"\\n=== RAW MODEL OUTPUT ===\\n\", raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c449085e-f03e-445a-af1c-094465b33902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================ FINAL RAW OUTPUT INSPECTION ================\n",
      "\n",
      "================================================================================\n",
      "ü©∫ RAW OUTPUT TEST 1\n",
      "TRANSCRIPT: Fever and cough for 3 days. Mild shortness of breath. Likely CAP. Order chest X-ray and start azithromycin 500 mg daily x5. Follow up in 2 days.\n",
      "\n",
      "=== RAW MODEL OUTPUT ===\n",
      " [\"fever and cough for 3 days. Mild shortness of breath. Likely CAP. Order chest X-ray and start azithromycin 500 mg daily x5. Follow up in 2 days.\"]\n",
      "================================================================================\n",
      "ü©∫ RAW OUTPUT TEST 2\n",
      "TRANSCRIPT: Left ankle pain after inversion injury yesterday. Likely lateral ankle sprain. X-ray ankle to rule out fracture. RICE and ibuprofen 400 mg PRN.\n",
      "\n",
      "=== RAW MODEL OUTPUT ===\n",
      " [\"left ankle pain after inversion injury yesterday. Likely lateral ankle sprain. X-ray ankle to rule out fracture. RICE and ibuprofen 400 mg PRN.\"]\n",
      "================================================================================\n",
      "ü©∫ RAW OUTPUT TEST 3\n",
      "TRANSCRIPT: Dysuria and urinary frequency for 2 days. No fever or flank pain. Likely uncomplicated UTI. Urinalysis and nitrofurantoin 100 mg BID x5 days.\n",
      "\n",
      "=== RAW MODEL OUTPUT ===\n",
      " [\"dysuria and urinary frequency for 2 days.\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n================ FINAL RAW OUTPUT INSPECTION ================\\n\")\n",
    "for i, txt in enumerate(demos, 1):\n",
    "    prompt = FEWSHOT.replace(\"{transcript}\", txt.strip())\n",
    "    raw_out = t5(prompt, **GEN_KW)[0][\"generated_text\"]\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ü©∫ RAW OUTPUT TEST {i}\")\n",
    "    print(\"TRANSCRIPT:\", txt)\n",
    "    print(\"\\n=== RAW MODEL OUTPUT ===\\n\", raw_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bd8bfed-5791-4837-927c-b408bee3fcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import OK. Example SOAP:\n",
      " S: ‚Äî\n",
      "O: ‚Äî\n",
      "A: ‚Äî\n",
      "P: ‚Äî\n",
      "Follow-up: ‚Äî\n"
     ]
    }
   ],
   "source": [
    "# This tests that your files can be imported *from disk*\n",
    "import importlib, sys\n",
    "for mod in [\"src.extract_clinical\", \"src.compose_note\"]:\n",
    "    if mod in sys.modules: del sys.modules[mod]\n",
    "\n",
    "ec = importlib.import_module(\"src.extract_clinical\")\n",
    "cn = importlib.import_module(\"src.compose_note\")\n",
    "\n",
    "assert hasattr(ec, \"extract_note\"), \"extract_note is missing in src/extract_clinical.py\"\n",
    "assert hasattr(cn, \"compose_note\"), \"compose_note is missing in src/compose_note.py\"\n",
    "\n",
    "note, raw = ec.extract_note(\"Fever and cough for 3 days. Follow up in 2 days.\")\n",
    "soap, summary = cn.compose_note(note)\n",
    "print(\"Import OK. Example SOAP:\\n\", soap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DocScribe",
   "language": "python",
   "name": "docscribe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
